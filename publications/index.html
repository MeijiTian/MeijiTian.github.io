<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Xuanyu Tian</title> <meta name="author" content="Xuanyu Tian"/> <meta name="description" content="publications by categories in reversed chronological order. * denotes the co-first author."/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/button.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://meijitian.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Xuanyu </span>Tian</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/friend/">Friends</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">publications by categories in reversed chronological order. * denotes the co-first author.</p> </header> <article> <div class="publications"> <h2 class="year">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 " style="text-align:center"> <abbr class="badge" style="background: var(--global-theme-color);border-style: solid;border-width: 2px;border-color:var(--global-theme-color)"> <div style="color:var(--global-bg-color);">MedIA 2025</div></abbr><br><p></p> <img id="JSMoCopng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/JSMoCo.png"><div id="JSMoCopng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('JSMoCopng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="JSMoCopng-modal-img"> </div> <script>var modal=document.getElementById("JSMoCopng-modal"),img=document.getElementById("JSMoCopng"),modalImg=document.getElementById("JSMoCopng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="chen2025joint" class="col-sm-8"> <div class="title">Joint coil sensitivity and motion correction in parallel MRI with a self-calibrating score-based diffusion model</div> <div class="author"> <a href="https://maopaom.github.io/" target="_blank" rel="noopener noreferrer">Lixuan Chen*</a>,  <strong>Xuanyu Tian*</strong>, <a href="https://miraiwu.github.io/" target="_blank" rel="noopener noreferrer">Jiangjie Wu</a>, Ruimin Feng, Guoyan Lao, <a href="https://scholar.google.com/citations?user=gIE0JTAAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Yuyao Zhang</a>, Hongen Liao, and <a href="https://scholar.google.com/citations?user=IORn-tEAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Hongjiang Wei</a> </div> <div class="periodical"> <i>Medical Image Analysis</i> <i>2025</i> </div> <div class="links"> <a href="https://arxiv.org/abs/2310.09625" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/MeijiTian/JSMoCo" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 " style="text-align:center"> <abbr class="badge" style="background: var(--global-theme-color);border-style: solid;border-width: 2px;border-color:var(--global-theme-color)"> <div style="color:var(--global-bg-color);">BSPC 2025</div></abbr><br><p></p> <img id="BSPC_2025png" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/BSPC_2025.png"><div id="BSPC_2025png-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('BSPC_2025png-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="BSPC_2025png-modal-img"> </div> <script>var modal=document.getElementById("BSPC_2025png-modal"),img=document.getElementById("BSPC_2025png"),modalImg=document.getElementById("BSPC_2025png-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="tian2025self" class="col-sm-8"> <div class="title">Self-supervised denoising for high-dimensional magnetic resonance image</div> <div class="author"> <strong>Xuanyu Tian</strong>, <a href="https://miraiwu.github.io/" target="_blank" rel="noopener noreferrer">Jiangjie Wu</a>, Guoyan Lao, Chenhe Du, Changhao Jiang, Yanbin Li, Jeff L Zhang, <a href="https://scholar.google.com/citations?user=IORn-tEAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Hongjiang Wei</a>, and <a href="https://scholar.google.com/citations?user=gIE0JTAAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Yuyao Zhang</a> </div> <div class="periodical"> <i>Biomedical Signal Processing and Control</i> <i>2025</i> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 " style="text-align:center"> <abbr class="badge" style="background: var(--global-theme-color);border-style: solid;border-width: 2px;border-color:var(--global-theme-color)"> <div style="color:var(--global-bg-color);">AAAI 2025</div></abbr><br><p></p> <img id="Spenerpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/Spener.png"><div id="Spenerpng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('Spenerpng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="Spenerpng-modal-img"> </div> <script>var modal=document.getElementById("Spenerpng-modal"),img=document.getElementById("Spenerpng"),modalImg=document.getElementById("Spenerpng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="tian2025unsupervised" class="col-sm-8"> <div class="title">Unsupervised Self-Prior Embedding Neural Representation for Iterative Sparse-View CT Reconstruction</div> <div class="author"> <strong>Xuanyu Tian</strong>, <a href="https://maopaom.github.io/" target="_blank" rel="noopener noreferrer">Lixuan Chen</a>, <a href="https://iwuqing.github.io" target="_blank" rel="noopener noreferrer">Qing Wu</a>, Chenhe Du, Jingjing Shi, <a href="https://scholar.google.com/citations?user=IORn-tEAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Hongjiang Wei</a>, and <a href="https://scholar.google.com/citations?user=gIE0JTAAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Yuyao Zhang</a> </div> <div class="periodical"> <i> Proceedings of the AAAI Conference on Artificial Intelligence</i> <i>2025</i> </div> <div class="links"> <a href="https://github.com/MeijiTian/Spener" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 " style="text-align:center"> <abbr class="badge" style="background: var(--global-theme-color);border-style: solid;border-width: 2px;border-color:var(--global-theme-color)"> <div style="color:var(--global-bg-color);">ICLR 2025</div></abbr><br> <abbr class="badge" style="background: var(--global-bg-color);border-style: solid;border-width: 2px;border-color:var(--global-theme-color)"> <div style="color:var(--global-theme-color);">Spotlight</div></abbr><p></p> <img id="Monerpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/Moner.png"><div id="Monerpng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('Monerpng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="Monerpng-modal-img"> </div> <script>var modal=document.getElementById("Monerpng-modal"),img=document.getElementById("Monerpng"),modalImg=document.getElementById("Monerpng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="wu2024moner" class="col-sm-8"> <div class="title">Moner: Motion Correction in Undersampled Radial MRI with Unsupervised Neural Representation</div> <div class="author"> <a href="https://iwuqing.github.io" target="_blank" rel="noopener noreferrer">Qing Wu*</a>, Chenhe Du*,  <strong>Xuanyu Tian</strong>, Jingyi Yu, <a href="https://scholar.google.com/citations?user=gIE0JTAAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Yuyao Zhang</a>, and <a href="https://scholar.google.com/citations?user=IORn-tEAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Hongjiang Wei</a> </div> <div class="periodical"> <i>The 13th International Conference on Learning Representations</i> <i>2025</i> </div> <div class="links"> <a href="https://arxiv.org/abs/2409.16921" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> </div> </div> </li> </ol> <h2 class="year">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 " style="text-align:center"> <abbr class="badge" style="background: var(--global-theme-color);border-style: solid;border-width: 2px;border-color:var(--global-theme-color)"> <div style="color:var(--global-bg-color);">MedIA 2024</div></abbr><br><p></p> <img id="Collatorgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/Collator.gif"><div id="Collatorgif-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('Collatorgif-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="Collatorgif-modal-img"> </div> <script>var modal=document.getElementById("Collatorgif-modal"),img=document.getElementById("Collatorgif"),modalImg=document.getElementById("Collatorgif-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="chen2024collator" class="col-sm-8"> <div class="title">COLLATOR: Consistent spatial-temporal longitudinal atlas construction via implicit neural representation</div> <div class="author"> <a href="https://maopaom.github.io/" target="_blank" rel="noopener noreferrer">Lixuan Chen*</a>,  <strong>Xuanyu Tian*</strong>, <a href="https://miraiwu.github.io/" target="_blank" rel="noopener noreferrer">Jiangjie Wu</a>, Guoyan Lao, <a href="https://scholar.google.com/citations?user=gIE0JTAAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Yuyao Zhang</a>, and <a href="https://scholar.google.com/citations?user=IORn-tEAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Hongjiang Wei</a> </div> <div class="periodical"> <i>Medical Image Analysis</i> <i>2024</i> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/S1361841524003219" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/maopaom/COLLATOR" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Longitudinal brain atlases that present brain development trend along time, are essential tools for brain development studies. However, conventional methods construct these atlases by independently averaging brain images from different individuals at discrete time points. This approach could introduce temporal inconsistencies due to variations in ontogenetic trends among samples, potentially affecting accuracy of brain developmental characteristic analysis. In this paper, we propose an implicit neural representation (INR)-based framework to improve the temporal consistency in longitudinal atlases. We treat temporal inconsistency as a 4-dimensional (4D) image denoising task, where the data consists of 3D spatial information and 1D temporal progression. We formulate the longitudinal atlas as an implicit function of the spatial-temporal coordinates, allowing structural inconsistency over the time to be considered as 3D image noise along age. Inspired by recent self-supervised denoising methods (e.g. Noise2Noise), our approach learns the noise-free and temporally continuous implicit function from inconsistent longitudinal atlas data. Finally, the time-consistent longitudinal brain atlas can be reconstructed by evaluating the denoised 4D INR function at critical brain developing time points. We evaluate our approach on three longitudinal brain atlases of different MRI modalities, demonstrating that our method significantly improves temporal consistency while accurately preserving brain structures. Additionally, the continuous functions generated by our method enable the creation of 4D atlases with higher spatial and temporal resolution.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 " style="text-align:center"> <abbr class="badge" style="background: var(--global-theme-color);border-style: solid;border-width: 2px;border-color:var(--global-theme-color)"> <div style="color:var(--global-bg-color);">MICCAI 2024</div></abbr><br><p></p> <img id="2024-lin-MICCAI-LFMRIpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/2024-lin-MICCAI-LFMRI.png"><div id="2024-lin-MICCAI-LFMRIpng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('2024-lin-MICCAI-LFMRIpng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="2024-lin-MICCAI-LFMRIpng-modal-img"> </div> <script>var modal=document.getElementById("2024-lin-MICCAI-LFMRIpng-modal"),img=document.getElementById("2024-lin-MICCAI-LFMRIpng"),modalImg=document.getElementById("2024-lin-MICCAI-LFMRIpng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="lin2024zero" class="col-sm-8"> <div class="title">Zero-Shot Low-Field MRI Enhancement via Denoising Diffusion Driven Neural Representation</div> <div class="author"> Xiyue Lin*, Chenhe Du*, <a href="https://iwuqing.github.io" target="_blank" rel="noopener noreferrer">Qing Wu</a>,  <strong>Xuanyu Tian</strong>, Jingyi Yu, <a href="https://scholar.google.com/citations?user=gIE0JTAAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Yuyao Zhang</a>, and <a href="https://scholar.google.com/citations?user=IORn-tEAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Hongjiang Wei</a> </div> <div class="periodical"> <i> International Conference on Medical Image Computing and Computer-Assisted Intervention</i> <i>2024</i> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 " style="text-align:center"> <abbr class="badge" style="background: var(--global-theme-color);border-style: solid;border-width: 2px;border-color:var(--global-theme-color)"> <div style="color:var(--global-bg-color);">IEEE TCI 2024</div></abbr><br><p></p> <img id="ZSD-HREMpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/ZSD-HREM.png"><div id="ZSD-HREMpng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('ZSD-HREMpng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="ZSD-HREMpng-modal-img"> </div> <script>var modal=document.getElementById("ZSD-HREMpng-modal"),img=document.getElementById("ZSD-HREMpng"),modalImg=document.getElementById("ZSD-HREMpng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="tian2024zero" class="col-sm-8"> <div class="title">Zero-Shot Image Denoising for High-Resolution Electron Microscopy</div> <div class="author"> <strong>Xuanyu Tian</strong>, Zhuoya Dong, Xiyue Lin, Yue Gao, <a href="https://scholar.google.com/citations?user=IORn-tEAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Hongjiang Wei</a>, Yanhang Ma, Jingyi Yu, and <a href="https://scholar.google.com/citations?user=gIE0JTAAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Yuyao Zhang</a> </div> <div class="periodical"> <i>IEEE Transactions on Computational Imaging</i> <i>2024</i> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2406.14264.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/MeijiTian/ZS-Denoiser-HREM" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>High-resolution electron microscopy (HREM) imaging technique is a powerful tool for directly visualizing a broad range of materials in real-space. However, it faces challenges in denoising due to ultra-low signal-to-noise ratio (SNR) and scarce data availability. In this work, we propose Noise2SR, a zero- shot self-supervised learning (ZS-SSL) denoising framework for HREM. Within our framework, we propose a super-resolution (SR) based self-supervised training strategy, incorporating the Random Sub-sampler module. The Random Sub-sampler is designed to generate approximate infinite noisy pairs from a single noisy image, serving as an effective data augmentation in zero-shot denoising. Noise2SR trains the network with paired noisy images of different resolutions, which is conducted via SR strategy. The SR-based training facilitates the network adopting more pixels for supervision, and the random sub-sampling helps compel the network to learn continuous signals enhancing the robustness. Meanwhile, we mitigate the uncertainty caused by random-sampling by adopting minimum mean squared error (MMSE) estimation for the denoised results. With the distinctive integration of training strategy and proposed designs, Noise2SR can achieve superior denoising performance using a single noisy HREM image. We evaluate the performance of Noise2SR in both simulated and real HREM denoising tasks. It outperforms state- of-the-art ZS-SSL methods and achieves comparable denoising performance with supervised methods. The success of Noise2SR suggests its potential for improving the SNR of images in material imaging domains.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 " style="text-align:center"> <abbr class="badge" style="background: var(--global-theme-color);border-style: solid;border-width: 2px;border-color:var(--global-theme-color)"> <div style="color:var(--global-bg-color);">Under Review</div></abbr><br><p></p> <img id="DPERpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/DPER.png"><div id="DPERpng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('DPERpng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="DPERpng-modal-img"> </div> <script>var modal=document.getElementById("DPERpng-modal"),img=document.getElementById("DPERpng"),modalImg=document.getElementById("DPERpng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="du2024" class="col-sm-8"> <div class="title">DPER: Diffusion Prior Driven Neural Representation for Limited Angle and Sparse View CT Reconstruction</div> <div class="author"> Chenhe Du, Xiyue Lin, <a href="https://iwuqing.github.io" target="_blank" rel="noopener noreferrer">Qing Wu</a>,  <strong>Xuanyu Tian</strong>, Ying Su, Zhe Luo, Rui Zheng, Yang Chen, <a href="https://scholar.google.com/citations?user=IORn-tEAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Hongjiang Wei</a>, S Kevin Zhou, Jingyi Yu, and <a href="https://scholar.google.com/citations?user=gIE0JTAAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Yuyao Zhang</a> </div> <div class="periodical"> <i> ArXiv preprint</i> <i>2024</i> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2404.17890" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Limited-angle and sparse-view computed tomography (LACT and SVCT) are crucial for expanding the scope of X-ray CT applications. However, they face challenges due to incomplete data acquisition, resulting in diverse artifacts in the reconstructed CT images. Emerging implicit neural representation (INR) techniques, such as NeRF, NeAT, and NeRP, have shown promise in under-determined CT imaging reconstruction tasks. However, the unsupervised nature of INR architecture imposes limited constraints on the solution space, particularly for the highly ill-posed reconstruction task posed by LACT and ultra-SVCT. In this study, we introduce the Diffusion Prior Driven Neural Representation (DPER), an advanced unsupervised framework designed to address the exceptionally ill-posed CT reconstruction inverse problems. DPER adopts the Half Quadratic Splitting (HQS) algorithm to decompose the inverse problem into data fidelity and distribution prior sub-problems. The two sub-problems are respectively addressed by INR reconstruction scheme and pre-trained score-based diffusion model. This combination first injects the implicit image local consistency prior from INR. Additionally, it effectively augments the feasibility of the solution space for the inverse problem through the generative diffusion model, resulting in increased stability and precision in the solutions. We conduct comprehensive experiments to evaluate the performance of DPER on LACT and ultra-SVCT reconstruction with two public datasets (AAPM and LIDC), an in-house clinical COVID-19 dataset and a public raw projection dataset created by Mayo Clinic. The results show that our method outperforms the state-of-the-art reconstruction methods on in-domain datasets, while achieving significant performance improvements on out-of-domain (OOD) datasets.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 " style="text-align:center"> <abbr class="badge" style="background: var(--global-theme-color);border-style: solid;border-width: 2px;border-color:var(--global-theme-color)"> <div style="color:var(--global-bg-color);">ISBI 2024</div></abbr><br><p></p> <img id="ISBI-2024-Laipng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/ISBI-2024-Lai.png"><div id="ISBI-2024-Laipng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('ISBI-2024-Laipng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="ISBI-2024-Laipng-modal-img"> </div> <script>var modal=document.getElementById("ISBI-2024-Laipng-modal"),img=document.getElementById("ISBI-2024-Laipng"),modalImg=document.getElementById("ISBI-2024-Laipng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="lai2024reconstructing" class="col-sm-8"> <div class="title">Reconstructing Knee CT Volumes from Biplanar X-Rays Via Self-Supervised Neural Field</div> <div class="author"> Shuyang Lai,  <strong>Xuanyu Tian</strong>, <a href="https://iwuqing.github.io" target="_blank" rel="noopener noreferrer">Qing Wu</a>, Chenhe Du, Xiaojun Xu, <a href="https://scholar.google.com/citations?user=IORn-tEAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Hongjiang Wei</a>, Xiaojun Guan, and <a href="https://scholar.google.com/citations?user=gIE0JTAAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Yuyao Zhang</a> </div> <div class="periodical"> <i> IEEE International Symposium on Biomedical Imaging (ISBI)</i> <i>2024</i> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="year">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 " style="text-align:center"> <abbr class="badge" style="background: var(--global-theme-color);border-style: solid;border-width: 2px;border-color:var(--global-theme-color)"> <div style="color:var(--global-bg-color);">ISBI 2023</div></abbr><br><p></p> <img id="ISBI2023png" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/ISBI2023.png"><div id="ISBI2023png-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('ISBI2023png-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="ISBI2023png-modal-img"> </div> <script>var modal=document.getElementById("ISBI2023png-modal"),img=document.getElementById("ISBI2023png"),modalImg=document.getElementById("ISBI2023png-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="jiang2023" class="col-sm-8"> <div class="title">Self-supervised High-Dimensional Magnetic Resonance Image Denoising Using Super-resolved Single Noisy Image</div> <div class="author"> Changhao Jiang*,  <strong>Xuanyu Tian*</strong>, Yanbin Li, <a href="https://miraiwu.github.io/" target="_blank" rel="noopener noreferrer">Jiangjie Wu</a>, Xin Mu, Lei Zhang, and <a href="https://scholar.google.com/citations?user=gIE0JTAAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Yuyao Zhang</a> </div> <div class="periodical"> <i> IEEE International Symposium on Biomedical Imaging</i> <i>2023</i> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Denoising of magnetic resonance image (MRI) is a critical step in MRI image processing and analysis. With the advantage of not requiring paired noisy-clean images for training, self-supervised denoising methods are emerging as competitive alternatives to supervised denoising methods in MRI denoising. However, current self-supervised image denoising methods are not effective enough for MRI. In this work, we propose Noise2SR-M (N2SR-M), a self-supervised denoising method for MR images, which is more efficient for high-dimensional MR images. N2SR-M is designed for training with paired noisy data of different sizes divided from a single high-dimensional noisy input image. Our N2SR-M model is able to utilize the redundant information from the additional image dimension to generate noisy image pairs for the denoising task. With the combination of additional dimension constraint and the effectiveness of SR method based training image pair generation, our model is more efficient for denoising high-dimensional MR images. The quantitative and qualitative improvements in blood oxygenation level dependent (BOLD) imaging denoising task demonstrate that N2SRM successfully restores detailed image contents and removes tiny structural noise and artifacts from noise-corrupted high-dimensional MRI. Moreover, the denoised BOLD image also induces more efficient R2* image computation.</p> </div> </div> </div> </li></ol> <h2 class="year">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 " style="text-align:center"> <abbr class="badge" style="background: var(--global-theme-color);border-style: solid;border-width: 2px;border-color:var(--global-theme-color)"> <div style="color:var(--global-bg-color);">MICCAI 2022</div></abbr><br><p></p> <img id="MICCAI2022_Noise2SRpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/MICCAI2022_Noise2SR.png"><div id="MICCAI2022_Noise2SRpng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('MICCAI2022_Noise2SRpng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="MICCAI2022_Noise2SRpng-modal-img"> </div> <script>var modal=document.getElementById("MICCAI2022_Noise2SRpng-modal"),img=document.getElementById("MICCAI2022_Noise2SRpng"),modalImg=document.getElementById("MICCAI2022_Noise2SRpng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="tian2022noise2sr" class="col-sm-8"> <div class="title">Noise2SR: Learning to Denoise from Super-Resolved Single Noisy Fluorescence Image</div> <div class="author"> <strong>Xuanyu Tian</strong>, <a href="https://iwuqing.github.io" target="_blank" rel="noopener noreferrer">Qing Wu</a>, <a href="https://scholar.google.com/citations?user=IORn-tEAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Hongjiang Wei</a>, and <a href="https://scholar.google.com/citations?user=gIE0JTAAAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Yuyao Zhang</a> </div> <div class="periodical"> <i> Medical Image Computing and Computer Assisted Intervention</i> <i>2022</i> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://link.springer.com/chapter/10.1007/978-3-031-16446-0_32" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://arxiv.org/pdf/2209.06411.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Fluorescence microscopy is a key driver to promote discoveries of biomedical research. However, with the limitation of microscope hardware and characteristics of the observed samples, the fluorescence microscopy images are susceptible to noise. Recently, a few self-supervised deep learning (DL) denoising methods have been proposed. However, the training efficiency and denoising performance of existing methods are relatively low in real scene noise removal. To address this issue, this paper proposed self-supervised image denoising method Noise2SR (N2SR) to train a simple and effective image denoising model based on single noisy observation. Our Noise2SR denoising model is designed for training with paired noisy images of different dimensions. Benefiting from this training strategy, Noise2SR is more efficiently self-supervised and able to restore more image details from a single noisy observation. Experimental results of simulated noise and real microscopy noise removal show that Noise2SR outperforms two blind-spot based self-supervised deep learning image denoising methods. We envision that Noise2SR has the potential to improve more other kind of scientific imaging quality.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Xuanyu Tian.Last updated: March 05, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>