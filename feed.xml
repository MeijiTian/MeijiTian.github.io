<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://meijitian.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://meijitian.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-05T21:49:36+08:00</updated><id>https://meijitian.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">D3N (IEEE Big Data 19) 论文阅读 - multi-layer cache size auto-configuration</title><link href="https://meijitian.github.io/paperreading/2022/11/05/D3N-(IEEE-Big-Data-19).html" rel="alternate" type="text/html" title="D3N (IEEE Big Data 19) 论文阅读 - multi-layer cache size auto-configuration"/><published>2022-11-05T00:00:00+08:00</published><updated>2022-11-05T00:00:00+08:00</updated><id>https://meijitian.github.io/paperreading/2022/11/05/D3N%20(IEEE%20Big%20Data%2019)</id><content type="html" xml:base="https://meijitian.github.io/paperreading/2022/11/05/D3N-(IEEE-Big-Data-19).html"><![CDATA[<p><a href="https://www.ccs.neu.edu/home/pjd/papers/ekaynar_bigdata19.pdf">D3N: A multi-layer cache for the rest of us</a></p> <p>networking + cache auto-configuration</p> <h2 id="abstract">Abstract</h2> <ul> <li>big-data jobs assume high bandwidth 大数据场景需要高带宽</li> <li>large network imbalances due to over-subscription and incremental networking upgrades 但是实际上整个网络架构并不如理想，<em>over-subscription</em>（比如 input 带宽加起来其实比 output 链路的能力要大很多，但是 networking 会假设 input 不容易全用满）以及网络可能不断加入新的输入（也是说 input 会越来越大 “<em>organic growth</em>”）</li> <li>给出来了一个 2-layer D3N cache (on Ceph RADOS Gateway)</li> </ul> <h2 id="introduction">Introduction</h2> <p>D3N = Datacenter-Data-Delivery Network</p> <ul> <li>D3N uses high-speed storage (e.g. NVMe flash or DRAM) to cache datasets <u>on the access side of links</u> in a hierarchical network, dynamically allocating cache space across layers based on observed workload patterns and link speeds, so that cache capacity is preferentially used for traffic crossing the most over-subscribed links.</li> </ul> <p>Questions:</p> <ul> <li>“D3N has required no changes to the interfaces of any Ceph services, involves no additional meta-data services (e.g, to locate cached blocks), and all policies are implemented based purely on local information.” really?</li> </ul> <h2 id="motivation">Motivation</h2> <p>这是架构图：</p> <figure> <picture> <img src="/assets/img/fig/D3N-fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>本文前面提到过 D3N cache 的设计是 2-layer 的，对着架构图来说，实际上就是“intra-rack”一层，”inter-rack“一层。</p> <p>为什么要这样呢？这里解释了 “differing upgrade schedules and split ownership typically <strong>prevent inter-cluster networks from being upgraded at the same time</strong>, resulting in significant <strong>bandwidth mismatches</strong> across compute clusters.” 所以除了需要 cache 来吸收请求，还需要 intra 和 inter cache size 的动态调整，来平衡请求（当然也不是万能的）。</p> <p>事实上例子也印证了直觉 “the multi-level approach offers better performance than either a pure L1 or pure L2 approach for 4 cache servers or more”：</p> <figure> <picture> <img src="/assets/img/fig/D3N-fig2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="d3n-architecture">D3N Architecture</h2> <ul> <li>For simplicity and resiliency, as well as for integration in existing storage solutions, all caching and routing decision are based on local information rather than central coordination.</li> </ul> <figure> <picture> <img src="/assets/img/fig/D3N-fig3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Each chunk (4 MiB) has <strong>a “home location” within the L2 cache</strong>, and L1 misses are forwarded to the chunk home location. Only in the event of a miss at the home location is a request forwarded to the data lake, the results of which are cached at both the home (i.e. L2) and client-serving (L1) locations. The L1 and L2 caches are unified: L1 requests for a chunk received at that chunk’s home location result in a single cached copy of the data.</p> </li> <li> <p>Limitations</p> <ul> <li>Local cache management: Since caching decisions are performed locally within the cache pools in each layer, D3N can make globally suboptimal caching decisions. For example, a few popular blocks can be replicated across all the L1 caches flooding the capacity and preventing caching of slightly less popular blocks.</li> <li>Lack of fairness: Compute clusters participating in D3N share resources such as rack space, storage, power, and network bandwidth with D3N. Even though D3N tries to provide a common good by eliminating network bottlenecks, the individual benefits each cluster gets from D3N may be different and disproportionate to the resources they provide.</li> </ul> </li> </ul> <h2 id="implementation-within-ceph">Implementation within Ceph</h2> <p>Questions:</p> <ul> <li>Figure6 没看懂</li> </ul> <p>略</p> <h2 id="evaluation">Evaluation</h2> <p>略</p> <h2 id="related-work">Related Work</h2> <p>略</p>]]></content><author><name></name></author><category term="paperreading"/><category term="Cloud"/><category term="Elasiticity"/><category term="Cache"/><summary type="html"><![CDATA[D3N: A multi-layer cache for the rest of us]]></summary></entry><entry><title type="html">CloudCache (FAST 16) 论文阅读 - Reuse Working Set (RSS) admission</title><link href="https://meijitian.github.io/paperreading/2022/10/29/CloudCache-(FAST-16).html" rel="alternate" type="text/html" title="CloudCache (FAST 16) 论文阅读 - Reuse Working Set (RSS) admission"/><published>2022-10-29T00:00:00+08:00</published><updated>2022-10-29T00:00:00+08:00</updated><id>https://meijitian.github.io/paperreading/2022/10/29/CloudCache%20(FAST%2016)</id><content type="html" xml:base="https://meijitian.github.io/paperreading/2022/10/29/CloudCache-(FAST-16).html"><![CDATA[<p><a href="https://www.usenix.org/conference/fast16/technical-sessions/presentation/arteaga">CloudCache: CloudCache: On-demand Flash Cache Management for Cloud Computing</a></p> <p>我关心 caching for cloud computing 有什么需要在意的，但是这篇主要是关注 Flash cache，而且还是 private cloud。</p> <h2 id="abstract">Abstract</h2> <p>目的是：</p> <ul> <li>meet VM cache demands</li> <li>minimize cache wear-out</li> </ul> <p>方法是：</p> <ul> <li>propose a new cache demand model “Reuse Working Set (RSS)” <strong>to capture only the data with good temporal locality</strong></li> <li>and use RSS size (RSSS) to model a workload’s <em>cache demand</em></li> <li><strong>predict RWSS online</strong> and admit only RWS into the cache</li> <li>propose a dynamic cache migration approach to balance cache load across hosts by live migrating cached data along with the VMs (both on-demand migration of dirty data and background migration of RWS)</li> <li>Support rate limiting on transfer to limit impact to <strong>co-hosted VMs</strong></li> <li>Evaluate on <strong>real-world traces</strong></li> </ul> <p>看到这里有的 Questions:</p> <ul> <li><del>如何识别 “data with good temporal locality”</del> 这里的意思就是 RWS</li> <li><del>什么是 “cache demand”</del> WS/RWS</li> <li>“online prediction” 开销如何</li> <li><del>migration 具体怎么做</del></li> <li><del>什么是 “impact to co-hosted VMs”</del> 因为要向外 migrate 数据到 co-hosted (Flash write)，所以有可能对 co-hosted 的性能有负面影响</li> <li><del>什么样的 “real-world traces”</del> several-month traces from their prior work</li> </ul> <h2 id="introduction">Introduction</h2> <p>新的 takeaway 是：</p> <ul> <li>the amount of flash cache that can be employed on a host is limited -&gt; 所以准入策略是重要的</li> <li>但如果数据就是都太重要了呢？migration！</li> </ul> <h2 id="motivation">Motivation</h2> <p>Questions:</p> <ul> <li><del>what does “consolidated systems” mean?</del> 可能是会有多个 VM 在一个 host</li> </ul> <h2 id="architecture">Architecture</h2> <figure> <picture> <img src="/assets/img/fig/CloudCache-fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Every host employs a flash cache, shared by the local VMs, and every VM’s access to its remote disk goes through this cache. A VM disk is rarely write-shared by multiple hosts (beyond this paper).</p> <p>CloudCache supports different write caching policies: (1) Write-invalidate; (2) Write-through; (3) Write-back</p> <figure> <picture> <img src="/assets/img/fig/CloudCache-fig2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Questions:</p> <ul> <li>SAN/IP-SAN 什么的完全不懂</li> </ul> <h2 id="on-demand-cache-allocation">On-demand Cache Allocation</h2> <h3 id="rws-based-cache-demand-model">RWS-based Cache Demand Model</h3> <p>Working Set, WS(t, T) at time t is defined as the set of distinct (address-wise) data blocks referenced by the workload during a time interval [t-T, t].</p> <p>-&gt;</p> <p>Reuse Working Set, RWS_N (t, T), which is defined as the set of distinct (address-wise) data blocks that a workload has reused at least N times during a time interval [t−T,t].</p> <p>对于本文的 workload in Table 1 以及 MSR Cambridge traces，N = 1 或者 2 才是最好的。（传统 cache algorithm N = 0）</p> <p>如果要使用 RWS 模型，需要解决两个新问题：</p> <ul> <li>How to track window? <ul> <li>real-time based window (x number of accesses made by the process，因为可能 VM 比较空闲的时候，非常长时间用不到 cache，这个 window 一直不切换，高估了 cache demand 也没有及时调整)</li> </ul> </li> <li>How to decide the size of the time window? <ul> <li>太大可能放了太多过去的情况进入考虑，太小可能低估了 cache demand</li> <li>profile 一段时间，根据绘制出来的图，选择 “knee point” size，如果没有 “knee point”，就选相对小的，下图例子应该选一个 24-48h 之间的数字</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/fig/CloudCache-fig3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="online-cache-demand-prediction">Online Cache Demand Prediction</h3> <p>要点如下：</p> <ul> <li>The success of RWSS-based cache allocation also depends on whether we can accurately <strong>predict</strong> the cache demand of the next time window <strong>based on the RWSS values observed from the previous windows</strong>.</li> <li>具体方法：To address this problem, we consider the <em>classic exponential smoothing and double exponential smoothing methods</em>. The former requires a smoothing parameter α, and the latter requires an additional trending parameter β . The values of these parameters can have a significant impact on the predic- tion accuracy. We address this issue by using the self tuning versions of these prediction models, which estimate these parameters based on the error between the predicted and observed RWSS values.</li> <li>下图是一个例子，值得提醒的是这个图我看了很久才完全理解对：首先 (a) 图的 WSS 曲线的几个 peak 都来自大型 scanning；其次 (b) 图为什么即使是 WSS 都数值不一样了呢？因为整个 (b) 图都是用了 smoothing method 的；(b) 图还想要说明的是 RWSS 其实有一些 “occasional bursts of IOs that do not reflect the general trend of the workload”，用 “RWSS+Filter” 效果更好，也确实可以看到一些小的凸起比如 day5 就没了。</li> </ul> <figure> <picture> <img src="/assets/img/fig/CloudCache-fig4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Questions:</p> <ul> <li>“exponential smoothing methods” 引入得好突然，不是很懂选它的前因后果</li> <li>虽然 RWSS+Filter 确实把突变点抹平了，但我不理解的是，为什么 occasional bursts of IOs 不值得这一天给他多分一点，确实这一天它有更多的需求啊，甚至不是那种来自 scanning 的需求（RWSS 已经过滤掉了只 hit 一次的情况）</li> <li>不懂这个 window 实际是怎么进行的：是缓慢滑行（pop &amp; push 那种感觉），还是单独 window 考虑（有提到用了 circular buffer）；这部分会不会影响性能？</li> </ul> <h3 id="cache-allocation-and-admission">Cache Allocation and Admission</h3> <ul> <li>The allocation of cache capacity should not incur costly data copying or flushing. <ul> <li>Hence, we consider <strong>replacement-time enforcement</strong> of cache allocation with logical partitioning at replacement time: a VM that has not used up its allocated share takes its space back by replacing a block from VMs that have exceeded their shares.</li> <li>If the cache is not full, the spare capacity can be allocated to the VMs proportionally to their predicted RWSSes or left idle to reduce wear-out.</li> </ul> </li> <li>Hybrid stage policy (address staging + data staging) <strong>in main memory</strong>: 需要考虑 RWS 或者 WS 都需要存历史信息；只存 address 的好处是不占空间 (8B address per 4KB data)，如果存 data 虽然挤占了存 address 的空间，但可以补偿一些 second miss —— 这里有个 trade-off</li> </ul> <p>Questions:</p> <ul> <li>不知道这里的 LRU 是怎么维护的，我好像 whole picture 不清晰</li> <li>in-memory 会不会价值更高？</li> </ul> <h3 id="evaluation">Evaluation</h3> <ul> <li>CloudCache is created upon block-level virtualization by providing virtual block devices to VMs and transparently caching their data accesses to remote block devices accessed across the network (Figure 1). <ul> <li>It includes <strong>a kernel module</strong> that implements the virtual block devices, monitors VM IOs, and enforces cache allocation and admission, and a user-space component that measures and predicts RWSS and determines the cache shares for the VMs.</li> <li>The kernel module <strong>stores the recently observed IOs in a small circular buffer</strong> for the user-space component to use, while the latter informs the former about the cache allocation decisions.</li> </ul> </li> </ul> <h4 id="prediction-accuracy">Prediction Accuracy</h4> <h4 id="staging-strategies">Staging Strategies</h4> <h4 id="wss-vs-rwss-based-cache-allocation">WSS vs. RWSS-based Cache Allocation</h4> <figure> <picture> <img src="/assets/img/fig/CloudCache-fig5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>No allocation = workload can use up the entire cache where the cache is large enough to hold the entire working set and does not need any replacement</p> <p>WSS 看上去也准入了 first miss，但比 No-Alloc 小很多是因为多了时间窗（比如 1 天的约束）。</p> <p>Questions:</p> <ul> <li><strong>怎么做到 No alloc 和 RWSS 明明 hit ratio 有一定差距，latency 却差不多的？</strong></li> </ul> <h2 id="dynamic-cache-migration">Dynamic Cache Migration</h2> <h3 id="live-cache-migration">Live Cache Migration</h3> <p>= move some VMs to other hosts</p> <p>结合以下两种方法：</p> <ul> <li>On-Demand Migration</li> <li>Background Migration</li> </ul> <p>Questions:</p> <ul> <li>怎么把原 workload 分到实验配置的两个 host 之上的？</li> </ul>]]></content><author><name></name></author><category term="paperreading"/><category term="Cloud"/><category term="Elasiticity"/><category term="Cache"/><summary type="html"><![CDATA[CloudCache: CloudCache: On-demand Flash Cache Management for Cloud Computing]]></summary></entry><entry><title type="html">Skyplane (NSDI 23) 论文阅读 - save egress-cost and maximize transfer speed</title><link href="https://meijitian.github.io/paperreading/2022/10/23/Skyplane-(NSDI-23).html" rel="alternate" type="text/html" title="Skyplane (NSDI 23) 论文阅读 - save egress-cost and maximize transfer speed"/><published>2022-10-23T00:00:00+08:00</published><updated>2022-10-23T00:00:00+08:00</updated><id>https://meijitian.github.io/paperreading/2022/10/23/Skyplane%20(NSDI%2023)</id><content type="html" xml:base="https://meijitian.github.io/paperreading/2022/10/23/Skyplane-(NSDI-23).html"><![CDATA[<p><a href="https://arxiv.org/abs/2210.07259">Skyplane: Optimizing Transfer Cost and Throughput Using Cloud-Aware Overlays</a></p> <p>想读的原因基本在 abstract 里了：multi-cloud、egress cost、user-provided constraint。</p> <h2 id="abstract">Abstract</h2> <p>关注 wide-area bulk data transder 的<strong>吞吐</strong>（性能）especially <strong>inter-region</strong></p> <p>解决方法：</p> <ul> <li>adapt network <strong>overlays</strong> (routing data through indirect paths <u>at the application layer</u>)</li> <li>consider trade-off between price (i.e. <strong>egress cost</strong>，传数据出云的花费) and performance</li> <li>use mixed-integer linear programming (for optimal overlay path and resource allocation)</li> <li>consider user-provided constraints on price or performance</li> </ul> <h2 id="introduction">Introduction</h2> <p>“Increasingly, cloud applications transfer data across datacenter boundaries, both across multiple regions within a cloud provider (<strong>multi-region</strong>) and across multiple cloud providers (<strong>multi-cloud</strong>). This is in part due to <u>privacy regulations, the availability of specialized hardware, and the desire to prevent vendor lock-in</u>. In a recent survey [26], more than 86% of 727 respondents had adopted a multi-cloud strategy across diverse workloads.”</p> <p>本文同时关注 multi-region 和 multi-cloud，并且给出了几类 motivation 来源。至于那个 survey [26]，我持保留意见，据我所知，multi-cloud 仍然 too far to be used。</p> <p>“How can we optimize network cost and throughput for cloud bulk transfer?”</p> <p>为什么不能优化在 routing protocols in internal networks of cloud providers？</p> <ul> <li>为 high throughput 优化的协议可能损害 low latency</li> <li>云服务商之间并没有兴趣做自己的数据“传出”的优化 “cloud providers lack a strong incentive to optimize dara transfer to other clouds”</li> </ul> <p>[定义] <em>Overlay paths</em>: paths that send data via intermediate regions</p> <p>Overlay path 有可能更<strong>快</strong>，因为这跟每两点之间的 bandwidth 支持有关。</p> <p>本文超越 overlay 这个老概念的地方在于 price 和 elasticity</p> <ul> <li>引入价格之后，bandwidth 不再是唯一的指标（<strong>richer problem space</strong>），如图：</li> </ul> <figure> <picture> <img src="/assets/img/fig/Skyplane-fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>点与点之间的 bandwidth 不是固定的，比如增加该 region 的 VM 数量就可以提升传输的能力（<strong>richer solution space</strong>）</li> </ul> <p>虽然有了更复杂的 problem/solution space，本质上还是解出 data transfer path &amp; amount of cloud resources to allocate along that path。</p> <p>Question:</p> <ul> <li><del>不明白怎么做到比原生支持的传输要快 X 倍。难道是因为可以通过 overlay 增加 bandwidth？</del> 是的，比如多开 VM 用来传输</li> </ul> <h2 id="background">Background</h2> <ul> <li>Network overlays: 这个概念专指 application 层面的 routing，此前与此相关的工作也各有各的优化目标。</li> <li>云服务商会有 service limits。每种 instance type 会有对应的 bandwidth 约束，egress bandwidth 有可能也有一个比例上限。</li> <li>egress cost 只跟量有关，跟速率无关。</li> <li>multi-region 的 <strong>intra-cloud transfers</strong> 价格越远越高，但 multi-cloud 的 <strong>inter-cloud transfers</strong> 与地理远近无关。</li> <li>Egress prices <strong>dominate</strong> the cost of a bulk transfers. 比如，比较的是租一个 VM 和传大量数据的 egress cost。</li> </ul> <p>我感觉以上的现状都会影响 pricing model 具体如何设计。</p> <ul> <li>Cloud object storage 也有自己的特点，例如：data stored immutably，consistency model各大云服务商都不同，读速率也有可能被约束</li> </ul> <h2 id="overview-of-skyplane">Overview of Skyplane</h2> <p>优化只支持两种选择：</p> <ul> <li>bandwidth subject to a price ceiling</li> <li>price subject to a bandwidth floor</li> </ul> <h3 id="overlay-formulation-in-skyplanes-planner">Overlay formulation in Skyplane’s planner</h3> <p>这里引入了一个假设 high statistical multiplexing: Single user’s bandwidth usage is negligible, thus Skyplane can compute a data transfer plan without regard to other users’ bulk transfer.</p> <h3 id="profiling-cloud-networks">Profiling cloud networks</h3> <p>两张图信息量足够：</p> <figure> <picture> <img src="/assets/img/fig/Skyplane-fig3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/fig/Skyplane-fig4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>“it should be sufficient to profile networks <strong>relatively infrequently</strong> (i.e. every few days). In practice, this information could be collected by <strong>third-party service</strong>, or measured via active probing along live transfers.”</p> <h3 id="skyplanes-data-plane">Skyplane’s data plane</h3> <p><strong>不明白这段用意是啥</strong></p> <h2 id="principles-of-skyplanes-planner">Principles of Skyplane’s planner</h2> <figure> <picture> <img src="/assets/img/fig/Skyplane-fig5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Question:</p> <ul> <li>“Note that the planner is not directly programmed to use these techniques; they are merely patterns that it discovers in the course of finding the optimal MILP solution.” ?</li> </ul> <h3 id="achieving-low-instance-and-egress-costs">Achieving low instance and egress costs</h3> <h4 id="choosing-the-relay-region">Choosing the relay region</h4> <h4 id="combining-multiple-paths">Combining multiple paths</h4> <h3 id="parallel-tcp-for-high-bandwidth">Parallel TCP for high bandwidth</h3> <p>经验上放 up to 64 TCP connections</p> <p>本文有一个关于 “fair share” 的讨论。</p> <h3 id="multiple-vms-for-high-bandwidth">Multiple VMs for high bandwidth</h3> <p>Skyplane’s planner takes into account a limit on the number of instances that a user can allocate per region.</p> <h2 id="finding-optimal-transfer-plans">Finding optimal transfer plans</h2> <p>Question:</p> <ul> <li><del>如果目标是 minimize cost，有什么情况会比直接点到点传输的 egress cost 要便宜？</del> compression or 每个 provider 的不同结点也可能不一样</li> </ul> <p>Objective（这段分析很有意思）：</p> <figure> <picture> <img src="/assets/img/fig/Skyplane-snapshot1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>不对称性：”We can approximate a solution by solving for the minimum cost transfer plan at a range of many throughput goals. The result of this procedure is a Pareto frontier curve (as shown in Fig. 9c). A throughput maximizing solution can be extracted from this curve. The quality of approximate solution will depend on how many samples are used.”</p> <h2 id="implementation-of-skyplane">Implementation of Skyplane</h2> <p>考虑了很多细节：</p> <ul> <li>使用 Gurobi library solver，但 Coin-OR library 也能用；</li> <li>AWS 选用 m5.8xlarge instances 而非更小的 VM 是因为更小的 VM 往往有 burstable networking performance；其他（Microsoft Azure, Google Cloud）为了一致性选用了差不多大的；</li> <li>To minimize unnecessary <strong>bloat</strong> in VM images, we use compact OSes such as Bottlerocket [3] and package dependencies via Docker（<strong>没太懂</strong>）；</li> <li>假设可以分成差不多大的小块；</li> <li>为了避免 straggler 效应，不使用现有的 round-robin 工具（GridFTP），而是看情况动态分发数据到各 TCP connections（我的理解是 Round-Robin 是 static 策略，有可能和预测不符的时候并不能自调整）；</li> <li>还要注意避免 overflow buffers at relay regions：use hop-by-hop flow control to stop reading data。这里提到了 bufferbloat problems（可能导致延迟很大）并没有被太考虑，因为 Skyplane 在意吞吐。</li> </ul> <h2 id="evaluation">Evaluation</h2> <p>Question:</p> <ul> <li>“In certain cases, Azure AzCopy performs about as well as Skyplane. We chose the koreacentral region because we expected the greatest improvements from the overlay in that region; however, storage overheads (the “thatched” regions of the bars), not networking overheads, dominated the runtime. It is possible that AzCopy avoids the Azure Blob Storage I/O overhead that dominates Skyplane’s transfer time by leveraging Azure’s Copy Blob From URL API call to download data directly into the servers running Azure Blob Storage [11].” 好像是说内部已经有优化做的比较好了</li> <li>Figure7 的 egress limit 是怎么设置的？为什么 limit 是一个 bandwidth？</li> <li>Skyplane w/o overlay 比 baseline 多了些什么来着？price model？</li> </ul> <p>这里定义了一个 <em>transfer bottleneck</em> (utilization is over 99%)，在整个链路上可以不止一处满足 bottleneck 定义。Skyplane 看到 source link bottlenecks 减少，另一边 source VM &amp; overlay link 上升。</p> <p><strong>没看懂这几个 baseline 的情况</strong></p> <h2 id="评论">评论</h2> <p>本文整体有倾向是多花一点钱，能提升极大的 bandwidth，其实并不能怎么关注省钱。</p>]]></content><author><name></name></author><category term="paperreading"/><category term="Cloud"/><summary type="html"><![CDATA[Skyplane: Optimizing Transfer Cost and Throughput Using Cloud-Aware Overlays]]></summary></entry><entry><title type="html">SPANStore (SOSP 13) 论文阅读 - span multiple cloud for cost-saving and low latency</title><link href="https://meijitian.github.io/paperreading/2022/10/15/SPANStore-(SOSP-13).html" rel="alternate" type="text/html" title="SPANStore (SOSP 13) 论文阅读 - span multiple cloud for cost-saving and low latency"/><published>2022-10-15T00:00:00+08:00</published><updated>2022-10-15T00:00:00+08:00</updated><id>https://meijitian.github.io/paperreading/2022/10/15/SPANStore%20(SOSP%2013)</id><content type="html" xml:base="https://meijitian.github.io/paperreading/2022/10/15/SPANStore-(SOSP-13).html"><![CDATA[<p><a href="https://dl.acm.org/doi/10.1145/2517349.2522730">SPANStore: cost-effective geo-replicated storage spanning multiple cloud services</a></p> <p>这篇文章的核心是通过 span multiple cloud provides 获得两个好处：1) increase <strong>geographical density</strong> of data centers; 2) minimze cost by <strong>exploiting pricing discrepancies</strong> across providers</p> <p>补充：选择空间从几个到了20个这种感觉。</p> <h2 id="abstract">Abstract</h2> <p>从这里看到的几个要点：</p> <ul> <li>storage services in geographically distributed data centers</li> <li>minimize cost（和纯 performance 视角不同）</li> <li>trade off replication with ‘higher storage’ and <strong>data propagation costs</strong> (i.e. transfer cost), 还需要满足 fault tolerance and consistency requirements</li> <li>also optimize ‘compute resources’ used on tasks such as two-phase locking and data propagation</li> </ul> <p>到这里有几个疑问在我心里：</p> <ul> <li><del>geographically distributed data centers 有哪些在意的维度？目前看来至少有 cost, latency, fault tolerance, and consistency</del> Yes, in this paper</li> <li>什么是 higher storage</li> <li><del>什么是 ‘compute resources’ used on tasks such as two-phase locking and data propagation</del></li> </ul> <p>看到目前觉得这篇文章考虑的视角并不狭窄（我不敢确定是不是足够全面），适合刚入门（比如我）学习。我是冲着 ‘exploiting pricing discrepancies’ 和 ‘data propagation costs’ 来的 :)</p> <h2 id="introduction">Introduction</h2> <p>首先记录一下这里提到的常见云服务（Amazon S3, Google Cloud Storage (GCS), Microsoft Azure），之后读近年的文章可能会看到不同的 set :)</p> <p>通过 PUT 和 GET 处理数据（PS: 据说近几年也有加入多一些基础 Operation 的趋势）。为了保证比较好的延迟，application 需要提供用户数据时最好从距离用户近的 data center 取。</p> <p>challenges (complexity):</p> <ul> <li>云服务商把操作 replication 的责任交给 application，他们只管给 ‘an isolated pool of storage’（这里默认需要 ‘replication’ 是因为数据如果不在多个服务器上都有备份，谈何从最近的里面取呢？）</li> <li>如果干脆不管了，就每个 data center 都有一份全部的数据呢？性能（latency for this paper）确实没毛病，但是太贵了，很多 application 还是希望刚好满足性能要求的情况下最便宜的方案；更何况数据和 client 都有自己的特征，全部来一份完整的显然太多冗余/浪费</li> <li>云服务商把责任交给 application 是合理的，因为服务商的 level 很可能会缺失 hint / semantics / knowledge 等</li> </ul> <p>Mark: SPANStore = Storage Provider Aggregating Networked Store</p> <p>Three key principles:</p> <ul> <li>span cloud providers（地理上看可选的 data center 更多了，还可以根据价格差异选最便宜的）；</li> <li>对于 Key-Value Store，哪些 object 要放在哪，以及从哪传输出来，都是责任； <ul> <li>影响因素：workload, latency requirement, fault tolerance, consistency requirement, pricing model（和之前完全对应）</li> </ul> </li> <li>第三点的细节<strong>没看懂</strong>：To keep costs low, we ensure that all data is largely exchanged directly between application virtual machines (VMs) and the storage services that SPANStore builds upon; VMs provisioned by SPANStore itself rather than by application provider are predominantly involved only in metadata operations.</li> </ul> <p><del>Mark: 号称有两个 killer application</del> 最后有讲到</p> <p>Question:</p> <ul> <li>这些云服务的特点和差异都是什么呢？</li> <li><del>后面会有完整的云服务商价格差异表吗？</del> 现在情况如何了呢？</li> <li><del>和 single cloud 的核心区别在哪？single cloud的这种系统同样考虑 ‘workload, latency requirement, fault tolerance, consistency requirement’，这篇文章的贡献只是多了 ‘pricing model’？</del> Figure 9</li> </ul> <p>lecture 里面放了一张图：</p> <figure> <picture> <img src="/assets/img/fig/SPANStore-fig3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>关于云服务商的背景，推荐一波 <a href="https://sigops.org/s/conferences/hotos/2021/papers/hotos21-s02-stoica.pdf">From Cloud Computing to Sky Computing</a> [HotOS 21]，有讨论 cloud 的一些特点。</p> <h2 id="problem-formulation">Problem formulation</h2> <p>跨云服务商的讨论，只考虑 <strong>storage</strong> 不考虑 <strong>computing</strong>（since cloud computing platforms vary in the abstractions）</p> <p>目标：</p> <ul> <li>cost (both storage cost and compute cost)</li> <li>latency</li> <li>fault tolerance</li> <li>consistency</li> </ul> <p>Challenges（从写作角度看写得挺 convincing）：</p> <ul> <li>Inter-dependencies between goals</li> <li>Dependence on workload <ul> <li>一个有趣的例子：to reduce network transfer costs, it is more cost-effective to replicate the object more (less) if the workload is dominated by GETs (PUTs).</li> </ul> </li> <li>Multi-dimensional pricing <ul> <li>维度很多：存储数据的量，PUT &amp; GET 的价格，transfer data 的量多不多（以及传到多远）</li> <li>Note: transfer cost 和 PUT/GET cost 不同</li> </ul> </li> </ul> <h2 id="why-multi-cloud">Why multi-cloud</h2> <p>Lower latencies &amp; Lower cost</p> <p>从写论文角度看，这个图画的不错</p> <figure> <picture> <img src="/assets/img/fig/SPANStore-fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Question: <strong>为什么 a &amp; b 图（c &amp; d）的红色长得不一样？</strong></p> <h2 id="overview">Overview</h2> <figure> <picture> <img src="/assets/img/fig/SPANStore-fig2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Mark: PMan = Placement Manager</p> <p>这图信息量挺大。</p> <p>这里提到一个细节：</p> <ul> <li>Divide time into fixed-duration <strong>epochs</strong>; an epoch lasts one hour in current implementation</li> </ul> <p>这是系统文章常见的操作——总是存在“magic number” :)</p> <h2 id="determining-replication-policies">Determining replication policies</h2> <h3 id="inputs-and-output">Inputs and output</h3> <p>首先，这里终于解释了 consistency：</p> <ul> <li>To capture consistency needs, we ask the application developer to choose between <strong>strong</strong> and <strong>eventual</strong> consistency.</li> <li>In the strong consistency case, we provide <strong>linearizability</strong>, i.e., all PUTs for a particular object are ordered and any GET returns the data written by the last committed PUT for the object.</li> <li>In contrast, if an application can make do with eventual consistency, SPANStore can satisfy lower latency SLOs. Our algorithms for the eventual consistency scenario are extensible to other consistency models such as causal consistency [26] by augmenting data transfers with additional metadata.</li> </ul> <p>留下关于 eventual consistency 的疑问，可能需要读 citation（[26] Don’t settle for eventual: Scalable causal consistency for wide-area storage with COPS [SOSP 11]）</p> <p>第二，提出了 <em>Access set</em> 概念: 每个 object 会被哪些 data center 访问（无信息也可以设置为 all），但是本文假设是有这个信息的。</p> <p>Questions:</p> <ul> <li><del>感觉 objects grouped 之后 metadata 还是太多了吧？还是说如果有 N 个 data center，有 2^N 种分类而不是每个 object 有 access set list？</del> 是后者。</li> </ul> <p>第三，利用 diurnal and <strong>weekly</strong> patterns [11, 17]</p> <p><strong>stationary</strong>: 关于 weekly，分析每个<strong>小时</strong>（也是每个epoch）与一周前的那个小时的 relative difference，发现虽然单用户指标不是很好看，但是当 “consider those users whose timelines have their access set as all EC2 data centers in US”<strong>（没懂这个 category 是怎么分的）</strong>，relative difference 恒小于 50%。</p> <p>intuitively: 某个用户不一定稳定在周一早上 7 点触发 GET request，但是用户群整体可能有相对稳定（50% relative difference in this paper）的访问量。</p> <p>好处：enable more accurate prediction based on historical workload measurements</p> <p>([11] Volley: Automated data placement for geo-distributed cloud services [NSDI 11], and [17] Characterizing, modeling, and generating workload spikes for stateful services [SoCC 10])</p> <p>第四，讲到了 replication policy：1）为每个 access set（会被相同的 data center 访问的一群 object）决定哪些 data center 拥有它们；2）为 access set 里的那些 data center 决定从哪里读写数据。</p> <p>Questions:</p> <ul> <li><del>access set 顶多是一种访问权限的组，为什么有必要/有好处作为一个组来考虑 replication 呢？</del> 我觉得是为了 simplicity，不然复杂度更高。</li> </ul> <h3 id="eventual-consistency">Eventual consistency</h3> <p>不让所有 data center 拥有全部的数据的原因：1）storage cost 太高；2）PUT 需要对所有备份做一遍，PUT request cost 太高。</p> <p>Solution: determine the replication policy for a given access set AS as a mixed integer program.（整型规划，这里要解的是0/1规划：whether a data center is GET/PUT replica）</p> <p>Appendix A:</p> <ul> <li>因为 fault tolerance = f，所以要有 f+1 个 replica（这里是说有 f+1 个备份就可以支持 f 个 data center 里内容其实不是 up-to-date 的，倒不是说 corruption）；</li> <li>因为 PUT/GET replica set 不需要重合也确实不重合，所以它们的并集内的 data center 都有全部的 object；</li> <li>对于 eventual consistency，PUT 只需要同步到某一个 replica，之后都异步做即可；</li> <li>有时候多几跳（relay via other data centers）反而比直接传输能节省 networking cost。</li> </ul> <p>Question:</p> <ul> <li>Assume PUT 可以多几跳，但是 GET 是直接从 replica 获得，为什么不考虑 relay 呢？有意思的 Related work: <a href="https://arxiv.org/pdf/2210.07259.pdf">Skyplane: Optimizing Transfer Cost and Throughput Using Cloud-Aware Overlays</a></li> </ul> <h3 id="strong-consistency">Strong consistency</h3> <p>rely on quorum consistency</p> <p><strong>暂略（包括了 Appendix B）</strong></p> <h2 id="spanstore-dynamics">SPANStore dynamics</h2> <h3 id="metadata">Metadata</h3> <p>“At any data center A, SPANStore stores an (a) in-memory version mapping for objects stored at A. If the application is deployed at A, SPANStore also stores (b) the access set mapping for objects whose access set includes A, and (c) <strong>replication policy versions for different epochs</strong>.”</p> <p>“when serving the <strong>first operation</strong> for an object in a particular epoch, SPANStore needs to account for both the replication policy currently in use for that object and the new replication policy computed by PMan for the current epoch.”</p> <h3 id="serving-puts-and-gets">Serving PUTs and GETs</h3> <p>Eventual consistency: 获得了至少一个replica的回复（PUT/GET）</p> <p>Strong consistency: 2PL (PUT)</p> <ul> <li>需要等所有的replica</li> </ul> <p>Modified 2PL protocol: 1) PUT，不管锁，直接写，留下版本号；2) GET，读到最近版本号到即可</p> <h3 id="fault-tolerance">Fault tolerance</h3> <p>Eventual consistency: 利用存储层次的保障，应该不会丢数据吧（=啥也没有）</p> <p>Strong consistency: “quorum sets”</p> <h3 id="handling-workload-changes">Handling workload changes</h3> <p>如何识别 replication policy 切换后的 <strong>first operation</strong>？version mismatch！</p> <p>这里讨论了识别出来分别是 GET 和 PUT 要怎么处理（略，因为 intuitive）。</p> <p>Question:</p> <ul> <li><strong>但是 policy 的变化是某个 Access Set 在 Data center 之间整体迁移的大动作，这里凭什么只讨论 first operation 呢？</strong></li> </ul> <h2 id="implementation">Implementation</h2> <p>On Amazon S3, Microsoft Azure, and Google Cloud Storage</p> <p>补充一点我查到的背景知识：</p> <ul> <li><strong>XML-RPC</strong> is a remote procedure call (RPC) protocol which uses XML to encode its calls and HTTP as a transport mechanism.</li> <li><strong>The IBM ILOG CPLEX Optimizer</strong> solves integer programming problems, very large linear programming problems using either primal or dual variants of the simplex method or the barrier interior point method, convex and non-convex quadratic programming problems, and convex quadratically constrained problems (solved via second-order cone programming, or SOCP).</li> </ul> <p>一些关于具体实现的细节：</p> <ul> <li>“The client library exports two methods: GET(key) and PUT(key, value, [access set])”</li> <li>XMLRPC exports LOCK(key) RPC to acquire object-specific locks; RELAY(key, data, dst) to indirectly relay a PUT; receives replication policy updates fromm PMan</li> </ul> <p>Question:</p> <ul> <li><del>为什么 GET 和 PUT 的函数输入不一样？</del> 可能因为 PUT 有可能是新的</li> <li>“uses DNS to discover the local memcached cluster” 从何说起？</li> </ul> <h2 id="evaluation">Evaluation</h2> <ul> <li>cost savings</li> <li>cost-optimality of replication policies</li> <li>cost necessary for increased fault-tolerance</li> <li>scalability of PMan</li> </ul> <p>Note: application on EC2’s data centers; SPANStore’s storage services offered by S3, Azure, GCS.</p> <h3 id="cost-savings">Cost savings</h3> <p>关于 SLO 的设定，有意思的细节是：for GET，250 ms is minimum SLO with “single replica for every object”; for PUT, 830 ms is minimum SLO with “replication everywhere”.</p> <p>Figure 9 画的不好看清楚。</p> <p>我没有细看每条线的对比，我感觉和 Single or Everywhere 比，赢了很合理；和 single-cloud 比，应该是赢在 data center 选择更多和价格差异上。</p> <p>补充一下：这个工作不止是可以优化拓扑结构，也可以减少 replica 的绝对数量（选择多了，也许一个 DC 可以保证原先不得不用两个 DC 才能做到的 SLO）；cost 是 goal 的话，资源的节省有可能伴随 cost 产生。</p> <h3 id="impact-of-aggregation-of-objects">Impact of aggregation of objects</h3> <p>这里我觉得视角不够好。</p> <p>论文在 Figure 12 给出了 per-object 预测效果远不如 aggregate workload 的效果好，但这只够论证选 per-object 预测不可行，不能说明 aggregate 可行。当然，本节标题也只是说了 aggregation 很有效果。</p> <p>结合前面给过的一个 aggregate workload relative difference 50% 以内的数据，算是圆上了。</p> <h3 id="cost-for-fault-tolerance">Cost for fault tolerance</h3> <p><strong>略</strong></p> <h3 id="scalability-of-placementmanager">Scalability of PlacementManager</h3> <p>“PMan needs to compute the replication policy for all access sets; there are 2^N access sets for an application deployed across N data centers.”</p> <p>CPU 算力只能支持 15 及以下数量的 data center，但实际上不需要每个 epoch 都重新计算（workload 变了再说）。</p> <p>感觉确实不是很 scalable，虽然对当时的场景够用了。</p> <h2 id="case-studies">Case studies</h2> <p>killer application 来了！</p> <ul> <li>Retwis is a clone of the Twitter <strong>social networking</strong> service (w. eventual consistency)</li> <li>ShareJS is a collaborative document <strong>editing</strong> webservice (w. strong consistency)</li> </ul> <p>这部分有点少。只展示了两个 application （两种 consistency requirement）下 SPANStore 都可以“正确地”满足 SLO，但没有说省到钱没有（或者原先是怎么做的）。</p> <h2 id="related-work">Related work</h2> <ul> <li>Evaluating benefits of cloud deployments <ul> <li>视角不够全面</li> </ul> </li> <li>Using multiple cloud services <ul> <li>此前只考虑了：availability, durability, vendor lock-in, performance, and consistency（没有 cost）</li> <li>SPANStore “unify idea”</li> </ul> </li> <li>Optimizing costs and scalable storage <ul> <li>没能跨 data center 考虑问题</li> </ul> </li> <li>Low-latency geo-replicated storage with improved consistency <ul> <li>没有考虑 minimize cost</li> </ul> </li> </ul> <h2 id="评价">评价</h2> <ul> <li>没有开源</li> <li>虽然 related work 不够 convincing（比如我总觉得这种整型优化肯定有人做了的，但是却没有比较到），但是如果我是 PC，我会觉得这个工作还是填补了空缺的、值得 accept 的</li> <li>整篇看下来不细想的话没啥问题，但是一细想就发现很多细节都不知道是如何做的，或者有一些假设是有可能不合适的。</li> <li>不知道后续有没有足够有意思的 follow-up（比如应用在了哪里）。</li> <li>我还看见<a href="http://dsrg.pdos.csail.mit.edu/2013/09/30/spanstore/">一篇介绍 SPANStore 的文章</a>（来自 MIT）。一段有意思的评价：It seems that there is still a huge burden on developer to provide the correct inputs to PMan so that PMan can provide the best replication policy. This doesn’t seem to reduce the complexity involved. A lot of the paper relies on the objective function, and there are not many new distributed system concepts.</li> </ul>]]></content><author><name></name></author><category term="paperreading"/><category term="Cloud"/><summary type="html"><![CDATA[SPANStore: cost-effective geo-replicated storage spanning multiple cloud services]]></summary></entry><entry><title type="html">MSRA internship 经验分享</title><link href="https://meijitian.github.io/application2phd/2022/09/27/MSRA-internship%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB.html" rel="alternate" type="text/html" title="MSRA internship 经验分享"/><published>2022-09-27T00:00:00+08:00</published><updated>2022-09-27T00:00:00+08:00</updated><id>https://meijitian.github.io/application2phd/2022/09/27/MSRA%20internship%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB</id><content type="html" xml:base="https://meijitian.github.io/application2phd/2022/09/27/MSRA-internship%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB.html"><![CDATA[<h2 id="读者须知">读者须知</h2> <p>“被误解是表达者的宿命。”对于这个断言，我选择的应对策略是，看内容是由什么相关经历的人表达的，因为只有这样才能筛选后吸收。</p> <p>在本文中，我会尽力让与我做出的断言相关的论据更加明晰。“尽信书不如无书。”我也期待读者能自行从中选取适合参考的经验，并且不被其他内容干扰。那我们开始吧！</p> <h2 id="背景介绍">背景介绍</h2> <p>我于2021年7月中旬-2022年6月在 MSRA 实习，认识的人跨越了各个年级、各个组、各种职业规划、各种进入MSRA实习的方式。</p> <p>其中有价值的可能有：</p> <ul> <li>什么情况适合在 MSRA 实习？</li> <li>如何申请到 MSRA 的实习机会？</li> </ul> <h2 id="在-msra-实习的动机">在 MSRA 实习的动机</h2> <p>首先介绍下我是怎么选择来 MSRA 实习的：</p> <ul> <li>大四一年想离开合肥去其他地方感受一下，并且安排点工作而不是玩耍一年 <ul> <li>我查看了几个科技公司为大四学生定制的实习机会，从要求的各种知识就可以推测是工作导向型的，遂放弃；</li> <li>在国内大四学生想要找科研导向的机会，比较方便的除了各高校的实验室就是 MSRA 了 [欢迎评论区补充其他机会或者大四在高校实验室科研的感受，我也就知道 systems 的情况 :)]。我大三已经感受过 ADSL @ USTC 的氛围了，所以大四想去企业看看（虽然 MSRA 也不是典型的企业科研组）。</li> <li>想去北京玩一年 :)</li> </ul> </li> <li>MSRA 有<a href="https://www.microsoft.com/en-us/research/group/systems-and-networking-research-group-asia/">系统组</a>，并且科研氛围浓厚。虽然我到了 MSRA 才发现全是做 AI for Systems &amp; Systems for AI 的 :(，但这不是什么很难发现的事情，看publication即可，推荐大家去之前思考清楚自己适不适合这个组或者mentor 的科研方向。</li> <li>USTC 有去 MSRA 实习的传统（我们官方的渠道叫创新人才项目和联培博士项目，每年有固定名额），但实际上自己申请到实习的 head count 也是一条路（这个在“如何申请 MSRA 的实习机会”部分讨论）。</li> <li>我最最直接的原因是申请出国往往需要3封推荐信，我希望能有更多面的推荐信视角&amp;来源，MSRA 这种一直在国际科研前沿的地方获得的推荐信更有说服力。 <ul> <li>也许有人会想到，为什么不出国暑研？其实我联系过一些暑研，但我后来还是希望把我独立一作的工作坚持做完（现在看来至少对于申请是正确的选择），去跟别的组暑研两三个月很难有自己是一作的产出，如果新开一个自己的题目，也可能只够起步。导师们推荐我在 MSRA 继续我的项目，所以我主要是靠内推获得的 head count（当然我也通过了mentor的面试）。和之前的导师们继续合作也是我不太担心 MSRA 不适合我的原因，大不了就像在 ADSL 干活一样嘛 :)</li> </ul> </li> </ul> <p>总结：我需要一个合适的地方完成我自己的科研。MSRA 的科研自由度大、不同于实验室科研的氛围以及和 ADSL@USTC 的友好 connection 都是非常适合我的。</p> <p>那么其他实习生都是怎么选择的呢？其实我感觉，大部分人是清楚地知道这个组/mentor的科研是厉害的/有趣的/对自己的学术有促进作用的（而我一个做纯 Storage Systems 的在系统组里像一座孤岛…），或者他们是来 MSRA 做对口的偏工程项目的实习的。</p> <h2 id="如何申请到-msra-的实习">如何申请到 MSRA 的实习</h2> <ul> <li>提前几个月把自己的简历准备好和申请表一起投到申请池里。有一部分mentor喜欢在需要的时候从池里捞人出来面试。 <ul> <li>我从21年3月投了简历，一共收到过三次来自这个池的面试邀请（我填写的是2021.7-2022.6空闲）。第一次是21年四月，来自DKI组，希望找人干不那么research的工作，并且希望能5月入职，我考虑到想做科研并且还要在学校上课就拒绝了；第二次的时候我已经内推入职了，但是那个池里我的简历忘记请mentor删掉（也有可能mentor没法删掉，除非从系统里选择录取这个档案？），Systems 组的一位mentor 电话问我感不感兴趣，大概是21年9月份，我说我已经入职了；第三次是22年4月份，收到了另一位 Systems 组的mentor 邮件问我要不要面试，我说我已经入职了，并且推荐了正有空闲实习的其他同学（后来 ta 通过了面试）。</li> <li>优点是非常省事，并且是比较官方的渠道；</li> <li>缺点是不够快速和过于被动：对于有计划性的人，有可能要么不是research intern head count，要么不是想要的科研方向，最后已经把时间安排出去了才收到机会。</li> </ul> </li> <li>前面已经提到的”找内部人打听&amp;直接得到面试机会“<u>有可能</u>效率极高。 <ul> <li>我一共”推荐“过三位同学，基本都是打听到机会了让他们自己去面试（感觉还挺多mentor有head count的），对口&amp;背景不太差很容易通过。</li> <li>优点：效率很高，我个人很推荐这种方式</li> <li>缺点：不一定找得到人帮你找机会 :( 或者由于水平不行惨遭拒绝 :(</li> </ul> </li> <li>找导师内推 :) <ul> <li>这个就不用细说了</li> </ul> </li> </ul> <h2 id="最后">最后</h2> <p>不知道再写啥了。</p> <p>希望有帮助！</p>]]></content><author><name></name></author><category term="application2phd"/><category term="经验分享"/><summary type="html"><![CDATA[读者须知]]></summary></entry><entry><title type="html">第一次投稿经历</title><link href="https://meijitian.github.io/caching/2022/04/04/%E7%AC%AC%E4%B8%80%E6%AC%A1%E6%8A%95%E7%A8%BF%E7%BB%8F%E5%8E%86.html" rel="alternate" type="text/html" title="第一次投稿经历"/><published>2022-04-04T00:00:00+08:00</published><updated>2022-04-04T00:00:00+08:00</updated><id>https://meijitian.github.io/caching/2022/04/04/%E7%AC%AC%E4%B8%80%E6%AC%A1%E6%8A%95%E7%A8%BF%E7%BB%8F%E5%8E%86</id><content type="html" xml:base="https://meijitian.github.io/caching/2022/04/04/%E7%AC%AC%E4%B8%80%E6%AC%A1%E6%8A%95%E7%A8%BF%E7%BB%8F%E5%8E%86.html"><![CDATA[<h2 id="读者须知">读者须知</h2> <p>本文虽然打了“经验分享”的 Tag，但更像是个人体会的记录。读者了解了在常见投稿困难下，我的取舍带来的结果，可能会有一定感触。</p> <h2 id="前言">前言</h2> <p>本文的定位是”投稿“分享，所以将关注以下在投稿特有的问题：如何在有限时间保证一次<strong>完整的投稿</strong>。</p> <p>我也有自己没有做好的事情，例如：我给出的draft基本上只能涵盖必要信息，而没能组织成论文该有的水准；最后关头往往只能保证最重要的 evaluation，但距离尽善尽美的实现还有距离等等。这里就不展开了。</p> <p>最后我会记录一下从 5 个 <strong>Reviews</strong> 中学到的东西。</p> <h2 id="如何在有限时间保证一次完整的投稿">如何在有限时间保证一次完整的投稿</h2> <h3 id="writing-workflow">Writing workflow</h3> <p>虽然一直说成熟的 researcher 会提前开始写作，甚至利用写作 polish 工作本身 <a href="https://b23.tv/jOBW38O"><u>How to Write a Great Research Paper (bilibili)</u></a>，但初学者如我更可能面对的是最后几周赶稿。</p> <p>workflow 涉及了合作者之间修改的顺序、某些内容需要讨论的时候要迅速 call for meeting 并达成共识、某段落以及某合作者须在什么时间之前完成等等。</p> <p>为了能更好的写作，还需要尽早明确各种写法，包括：给 idea 和各个常用的部分选择称呼和简写、每个人的修改如何用各自的颜色或者签名标记、互相审阅完成如何标记等等。</p> <h3 id="安排实验时间">安排实验时间</h3> <ul> <li>在安排实验的时候，要预留容错时间，我一般自认为 1h 完成的工作，会安排 1.5h，这样往往刚刚好，在投稿前可以<strong>减轻焦虑</strong>心理</li> <li>自己构思出<strong>最高效的实验参数（最少的实验、最多面的信息量）</strong>，立刻与合作者/导师商量，达成一致再操作</li> <li>如果可以通过修改代码使得跑实验的流程更高效，这种前序工作是值得的（例如把测试脚本写得更好，比手动一个一个实验慢慢跑，要省心很多）。</li> </ul> <h2 id="reviews-收获">Reviews 收获</h2> <h3 id="需要多方面做实验验证">需要多方面做实验验证</h3> <ul> <li>重要参量的敏感度分析（避免 ad-hoc）</li> <li>synthetic / old traces 之外最好有强有力的 killer traces <ul> <li>比如 workload 只有新旧之分的时候，最开始就应该选择新的</li> <li>workload 的来源不同的时候，要按需选择。比如，caching 有 in-memory traces, block io traces 等等</li> </ul> </li> <li>仅仅口头说明好处不够有说服力，应该精心选择实现&amp;实验，在有限的篇幅内展示效果</li> <li>不要害怕做出不好的结果，这可能是好结果的前提</li> </ul> <h3 id="reviews-的不可替代性">Reviews 的不可替代性</h3> <p>我曾经有一个观点是：“至少我自己确认准备好了，才能去投稿；而如果我不确定项目好不好，那可能这不是一次成熟的投稿。”</p> <p>但这次投稿后，我发现很多视角（还有背景工作）可能是我自己不会想到的，再给我几个月可能也不会，反而我可能会滑向“闭门造车”。所以，在追求自己的完美目标和适时投稿之间，我相信存在一个极大值点。</p>]]></content><author><name></name></author><category term="caching"/><category term="经验分享"/><summary type="html"><![CDATA[读者须知]]></summary></entry><entry><title type="html">申请经验写什么</title><link href="https://meijitian.github.io/application2phd/2022/03/15/%E7%94%B3%E8%AF%B7%E7%BB%8F%E9%AA%8C%E5%86%99%E4%BB%80%E4%B9%88.html" rel="alternate" type="text/html" title="申请经验写什么"/><published>2022-03-15T00:00:00+08:00</published><updated>2022-03-15T00:00:00+08:00</updated><id>https://meijitian.github.io/application2phd/2022/03/15/%E7%94%B3%E8%AF%B7%E7%BB%8F%E9%AA%8C%E5%86%99%E4%BB%80%E4%B9%88</id><content type="html" xml:base="https://meijitian.github.io/application2phd/2022/03/15/%E7%94%B3%E8%AF%B7%E7%BB%8F%E9%AA%8C%E5%86%99%E4%BB%80%E4%B9%88.html"><![CDATA[<p>标题灵感来自于贞《耳朵听什么》:)</p> <h2 id="读者须知">读者须知</h2> <p>“被误解是表达者的宿命。”对于这个断言，我选择的应对策略是，看内容是由什么相关经历的人表达的，因为只有这样才能筛选后吸收。在本文中，我会尽力让与我做出的断言相关的论据更加明晰。“尽信书不如无书。”我也期待读者能自行从中选取适合参考的经验，并且不被其他内容干扰。那我们开始吧！</p> <h2 id="前言">前言</h2> <p>在动手写文之前，我一直在问自己的是“我该写什么？”，或者说“只有我来写才能写得好的是什么？”。为了帮助我思考，我回顾了吴永基学长等编写的《20届 CS 飞跃手册》，发现它大而全，囊括了申请要素的分析、各个CS方向的讨论、还有Gap or MS or AI 这些我不是很了解的话题，非常推荐阅读，<a href="https://adrain.ustclug.org/"><u>指路</u></a>。</p> <p>大而全也意味着，我所在的方向的分享只占一小部分。</p> <p>所以我打算以我的个人经历为开始，剖析在“Storage Systems”（属于 hard-core systems）方向及相似方向下，我的思考和选择。总的来说，我将重点写前人没太涉及的事情（<u>不重复造轮子</u>）。</p> <h2 id="个人经历-科研向">个人经历-科研向</h2> <p>书接上文“<a href="https://ustcqzy.github.io/2022/02/26/%E7%BB%99%E5%B0%91%E9%99%A2%E6%96%B0%E7%94%9F%E7%9A%84%E5%88%86%E4%BA%AB/"><u>给少院新生的分享</u></a>”，我将针对科研展开介绍我的本科经历。这里写了很多小故事，作用在于某个小故事也许恰好是学弟学妹会遇见的，希望有所帮助。我进实验室之前也听了很多小故事，感觉潜移默化地给了我很多铺垫 :)</p> <h3 id="选择-system">选择 System</h3> <p>出于三个原因：</p> <ol> <li>我不太想主动进入变迁速度过快的领域（AI, CV），这让我害怕会面临质量/数量的权衡，或者质量/速度的权衡，如果都想达到，那就得花费更多的精力。而 System 步调一直看起来比较稳健，顶会质量普遍很高，我只需要顺应内心做出好工作就可以。（当然，缺点是工作不容易发出来、毕业困难、产出辛苦……）</li> <li>我希望我所学的课程能在竞争中发挥作用。有些理论领域、AI领域不仅用不到操作系统、体系结构、编译原理，而且还需要和学数学专业的人竞争；CV 等交叉领域，我们也需要和有着交叉学科基础的同辈竞争。不考虑其他因素，system 更适合纯 CS 人。</li> <li>我希望能弄明白一切，基于 understanding 给出不一样的 solution。这对应着 System 的两个特点：弄清楚问题最重要，以及万事皆是 trade-off。</li> </ol> <p>当然，我也是个特别现实的人，客观条件要是不适合，我可能也会考虑其他的路。一般而言，科研还是非常难自学成才的，更何况是 system 这种领域。所以下一步就是选实验室，也就是选择科研环境。</p> <h3 id="选实验室">选实验室</h3> <p>我是大三上联系的导师，但是真正开始科研是等到了大三上结束的寒假（因为导师有点担心我一边上课一边科研会影响研究生们的工作规划）。我后来才得知，身边同实验室的同级学生，有的一开始科研就是自己开课题，遇到了或多或少的阻力。</p> <p>与此相反的是，因为我此前和许多各专业的学长学姐聊过，<u>天然地觉得第一段科研肯定要跟着师兄师姐多学一点</u>，开自己的项目是后面的事情。于是，第一次跟老师聊的时候，我就问“有没有什么课题或者哪位师兄师姐适合让我跟着做一段？”老师给了我两篇文章，让我感受一下哪篇更感兴趣，我选择了 SpanDB。</p> <p>这是我的第一个转折点：我遇到了非常“合适我”的师兄，带我走进了 storage 领域。</p> <h3 id="进入科研">进入科研</h3> <p>师兄所谓”合适“我的一点是：每当我出问题，他都会立刻指出来，并且告诉我怎么改。</p> <p>这样每次和师兄的交流，都是在解决我上一次还没暴露出来的问题，或者是说了一次还没能纠正的问题。这种感觉太棒了！我每一次都能感觉到我在进步。</p> <p>刚开始做科研的感觉是“晕头转向”：看不懂师兄写的文件系统的源码、不知道怎么减少 overhead、跑的实验参数也不能很好地理解意思、实验结果合不合理也是在一次次出问题听师兄分析后才领悟一些。</p> <p>但是，这也是我最轻松的一段时间。轻松在于，有任何不明白的都可以通过问师兄获得答案。</p> <p>很快我就没这么“好运”了：我们在探索的 caching 问题对于师兄来说也是新鲜的，当然也会有不明白、需要导师“掌舵”的时候，我于是开始参加每周的正式汇报。这是我的第二个转折点：我遇到了非常“合适我”的课题导师，带我真正地感受到存储的魅力。</p> <h3 id="导师">导师</h3> <p>因为有校外合作导师，我们都是线上开会。我领略到导师存在的魅力是从我正式组会开始。</p> <p>我跟导师的合作分为三个阶段：</p> <h4 id="初始阶段2月">初始阶段（2月）</h4> <p>刚开始的时候，我的任务是为师兄的系统提供性能优化，所以这和一般的科研进程不一样的是，问题“似乎”已经知道了，来自师兄告诉我的、他的观察：缓存性能不够好，看看我们可以做些什么。（但实际上由现在的我去回看，我会觉得当时我们并没有明确这个问题有创新性，也许跟当时目标是投期刊有关。）</p> <p>这阶段的工作流是</p> <p>（1）非常基础的一些实验数据的解释，我一般是咨询师兄。</p> <p>（2）等到开会的时候老师会问一些实验方向上的问题：<strong>为什么要这样设计实验</strong>？大部分时候我们会发现我们做的实验有冗余或者不足的地方，需要继续补充实验来做understanding的那些会转化为下一周的任务。<strong>这些实验数据可以怎么解读？</strong>（老师总是能发现我总结不出的视角，这让我不断期待下一次组会。）</p> <h4 id="试探阶段3-6月">试探阶段（3-6月）</h4> <p>每次组会后会反思和总结，有时候我会马后炮地发现“如何设计实验”这个问题的答案每一次和师兄不一致的时候我都会放弃思考，之后在组会发现老师的建议跟我想的一样的时候，我又会非常后悔。</p> <p>于是我开始主动思考，按我设计的思路走，只参考意见，但不会盲从。</p> <p>随着时间推移，我在组会上被问住的次数越来越少（因为我先行考虑了可能会有的疑问），我也越来越有信心。（现在的我已经明白了，这样的趋势是非常合理的。因为没有人能比我自己更懂得根据情况决定应该做什么。）</p> <p>这个阶段还有一个特别的地方：我们从诸多实验中提取了一个可能的新问题。我于是返回体验了科研开始阶段：明确问题的存在、重要性、以及使用已有的方法能不能解决（或削弱）它。</p> <p>导师们对我非常包容，给初入科研的我塑造了平和的科研心理和扎实的科研习惯。</p> <h4 id="进阶阶段7月-12月">进阶阶段（7月-12月）</h4> <p>这个阶段我经历了很多崩溃，但也是我进步最快的一段时光。</p> <p>师兄毕业了，我也来到了微软全职工作（之前我一周抽出两整天科研）。这时我猛然发现除了我自己，没有人可以在direction之外帮助我。是的，不会有人能付出巨大的commitment、花费大量时间帮我处理我的科研。这和课程作业咨询一道题的 commitment 可不一样。</p> <p>令我曾经痛苦不堪的事情包括：使用各种 application 做 baseline 的过程中给 github issue提问题（回复往往要等上几天）、代码不理解、实验结果不理解等等。随便什么事情就可以耗掉我几天。那时候我时常自嘲我自己进入了博士零年级。</p> <p>但也是这段时间我习惯了独立解决问题，用一切方法咨询能找到的人。之后有一次（大概是十一月）咨询师兄一个细节问题，我立刻得到<strong>“答案”</strong>的时候，猛然意识到我已经拥有独立科研的心态很久了。</p> <p>这段时间，老师们帮我提出的质疑越来越少，我为下周的任务排列的优先级也往往能得到认可。虽然中间几度觉得问题变得越来越不有趣，但是我从未从导师那里感受到“好失望啊”、“你怎么会这样呢”的情绪，在我低落的时候，他们会对我说“这没什么大不了的，我们还可以试试xxx”（后来我也常常对我的合作者说这句话 :) ）。</p> <p>我逐渐习惯于尽力自己解决问题，如果有没解决的，那只能因为还没有“时间”使用完所有方法，没有什么是我一定不能解决的。确定了核心课题之后，老师们会在我direction略有偏离的时候，坚定地把我拉回主道。也因为我自己排查的问题越来越多，在组会上讨论 idea 的时候更多了，我在思维上的进步的速度更快了。</p> <p>值得注意的是，组会的时间非常有限，老师在一小时能做的梳理工作非常有限。如果选择把一堆细节没有组织的情况下丢到组会里无差别介绍，只会是一团乱麻。</p> <h4 id="投稿后自由进阶阶段1月-present">投稿后自由进阶阶段（1月-present）</h4> <p>暂略，因为对申请没有很显著的影响（只对面试有微弱影响）。</p> <h4 id="总结">总结</h4> <p>好的导师可以带来非常正向的东西，我按优先级排列：</p> <ul> <li>平和的科研心理和扎实的科研习惯</li> <li>在任何时候（尤其是困境）有推动项目进展（促进创新）的能力</li> <li>提供科研和科研相关的各个方面（例如学生心理、生活）的后备支持</li> </ul> <p>从我讲述的故事里可以感受到在我众多的转折点中，导师的这些要素支持着我前进。虽然我确信如果失去这些要素的一些，我还会做科研，但是我一定不会有现在这样巨大的成长，也不会明白什么是好的科研氛围。</p> <h2 id="问题导向">问题导向</h2> <p>再强调一次这是针对system人（我自己）的回答，每个回答我都可以在不满足前提条件的情况下找到反例，所以注意辩证看待 :)</p> <h3 id="教授想要怎样的学生">教授想要怎样的学生？</h3> <p>简单来说是，<strong>证明了自己能做好科研的学生</strong>。</p> <p>可能因为system需要的计算机核心课多，等到准备好做科研的各种课程，已经大三快结束了，于是很多申请人都没有深入的科研经历。这是一把双刃剑。虽然很难有科研产出，但是如果有扎实、深入的科研背景，就很有区分度，可以甩掉一大批人。如果没有，可能就需要各显神通了，但对于陆本学生，感觉告别一批顶校了（欢迎各显神通的例子）。</p> <p>那么教授用什么来验证申请者能不能做好科研呢？领域内珍惜羽毛的人的强推 &gt; SOP » 其他（GPA，英语）。BTW，不确定不被熟悉的国内人的推荐，但有细节，效果如何（欢迎补充）。</p> <p>强推和写的好的SOP一般需要包括教授在乎的要素：<strong>如何克服了困难，从中体现了什么品质。</strong>（当然自己的推荐信里有啥是我们没法控制的，所以我先介绍下后者）</p> <p>下面是我们这方可以控制的：SOP与面试回答</p> <ul> <li> <p>比如，SOP 中我写过的故事有：想要用一个工具，发现有bug，定位到是什么问题并且report，一段时间后官方修复了问题，期间我使用plan B继续工作，没有被这个问题耽误进度、也跨过了一个常见困难。</p> </li> <li> <p>我在面试时有几次因为过于真实使得教授产生共鸣的：比如，问我现在的项目是不是考虑加一个xxx application，我（无奈）“是的，这是我evaluation plan的下一个任务，但我只有一个人，我目前还在明确related work有没有疏漏、视角是不是足够完善，等我过几周抽时间研究一个application。”然后教授说“哈哈哈，你已经挺好的了，我只是觉得reviewer会想要看这个。”</p> </li> <li> <p>再比如，教授说“我会希望你做一些big code base，你有信心吗？”我说：“我之前做 RocksDB 上的实现也很困难，因为关注的layer和application的结合让我需要理解不止特定一层layer。这也许不算很大的code base，但是我不觉得有什么是解决不了的，就是花两周还是三周这种区别而已。”教授貌似很满意我最后一句 :)</p> </li> </ul> <p>类推，想要推荐信里有这些要素，就得<strong>“好 好 做 科 研”</strong>。我本人曾经 impress 过导师的点有：</p> <ul> <li>主动思考我们的目标是什么，然后才去做最有必要的事情</li> <li>设计合理的实验参数，不做冗余实验</li> <li> <p>掌握 related work</p> </li> <li>给出扎实的实验数据（可以对每一个维度的疑问给出解释）</li> <li>上周卡住的问题，下周无论如何都会有一定突破（我会push自己）</li> </ul> <p>看起来像是在自夸，但这确实是常见的一些point。不为了推荐信，只是为了prepare自己，也需要提升各方面的科研素质。这个过程潜移默化丰富了推荐信和SOP的细节。</p> <h3 id="申请时各种材料如何发挥作用如何写好sop">申请时各种材料如何发挥作用？（如何写好SOP？）</h3> <p>重要的推荐信和 SOP 我前面已经提到了。还有次重要的材料，例如：托福 GRE、PS、GPA。</p> <p>我个人是比较 push 自己的，所以不重要的材料的自我要求都拉满了。</p> <p>给一个比较抽象的总结：欠缺“英语、PS、GPA”可能对个别学校、老师有影响，会让你<strong>失去一小部分offer</strong>；与之相对的是，强推会让你<strong>得到一大批offer</strong>。前者我可以举个几个例子：Cornell、Brown、UC系列，对英语有些要求，如果很差，可能直接被淘汰，老师再喜欢也没用 :(</p> <h3 id="如何拥有申请要素为什么大三暑假暑研是最普遍的现象">如何拥有申请要素？为什么大三暑假暑研是最普遍的现象？</h3> <p>这里的申请要素，可以认为指强推，也可以指获得强推的条件，如下：</p> <p>1 愿意给学生好好写推荐信的靠谱导师</p> <p>2 有前景的科研开端</p> <p>3 自己正确方向的努力带来一定产出等等。</p> <p>国外暑研一般能满足 1 和 2，并且可以帮助 3。但其实国内也可以有机会获得这三点，国外暑研也可以没有这三点，学生应该注意甄别和判断。</p> <p>关于第3点，自驱力是非常重要的，我也有见过导师很强但带不动学生的 :(</p> <p>综上，对于暑研，应该先明白为了什么，才能知道如何选择。</p> <h3 id="怎么选感兴趣的导师为找暑研申请时选校作调研">怎么选感兴趣的导师（为找暑研、申请时选校作调研）？</h3> <p>这里有个小trick，通过 <u>[CS Ranking](http://csrankings.org/)</u> 找近期有在你感兴趣的几个 conference 发 paper 的导师。这里有意思的是，同方向的导师存在聚类，比如我印象中 purdue 的 security 方向的教授多，而 UW-Madison 做 storage 方向的教授多。这样也可以辅助申请选校。</p> <p>我的经验是，方向match的导师在申请中，通常会对你有更大的兴趣；相反，不 match 的导师很有可能完全不感兴趣。暑研不同，因为这时情况不定、学生还在科研的初级阶段，教授对match的要求不高。但从match在申请中的影响可以看到：暑研如果做了不感兴趣的方向，极有可能从写推荐信的导师connection、到你的background都需要转换，对之后申请会有阻碍。<strong>暑研选择需慎重呀！</strong></p> <h3 id="套磁有哪些作用要做到什么地步">套磁有哪些作用？要做到什么地步？</h3> <p>套磁从加大录取概率上来说，用处不大。更何况申请材料提交前的套磁，缺失最重要的“推荐信”，这时就能下定决心录学生的教授还是挺少见的。</p> <p>但是套磁存在另一重用处：就是帮助选择中游和保底校，试探哪些学校、做哪些科研的人，会对你感兴趣。我确实是通过套磁回复，确定了保底校和主申的部分学校，但 Top 的学校几乎不回复套磁，想去就一定要申请，不然说不定会后悔哦！（比如我差点没敢申 CMU CSD，现在想起来一阵后怕 hhhh）</p> <h3 id="拿到-offer-开始反选后如何甄别判断如何跟意向导师聊天如何跟导师的学生聊天">拿到 offer 开始反选后，如何甄别判断？（如何跟意向导师聊天？如何跟导师的学生聊天？）</h3> <p>我使用这个里面的问题列表，包括了问老师的问题和问学生的问题。请视情况调整：<a href="https://blog.ml.cmu.edu/2020/03/02/questions-to-ask-a-prospective-ph-d-advisor-on-visit-day-with-thorough-and-forthright-explanations/"><u>Questions to Ask a Prospective Ph.D. Advisor on Visit Day</u></a></p> <h3 id="大四怎么安排">大四怎么安排？</h3> <p>相比物院某些专业大四上还有大学物理实验要上，科大计算机专业完全可以获得全空的一年，我大部分计算机的同学都用大四整块时间科研或者实习。具体操作是提前修够：1 专业选修的所有学分 + 2 公选4学分 + 3 剩余学分用任何不在培养计划上的课都可以补齐。</p> <h2 id="总结-1">总结</h2> <p>祝愿每个学弟学妹都走出自己的路！</p>]]></content><author><name></name></author><category term="application2phd"/><category term="经验分享"/><summary type="html"><![CDATA[标题灵感来自于贞《耳朵听什么》:)]]></summary></entry><entry><title type="html">给少院新生的分享</title><link href="https://meijitian.github.io/application2phd/2022/02/26/%E7%BB%99%E5%B0%91%E9%99%A2%E6%96%B0%E7%94%9F%E7%9A%84%E5%88%86%E4%BA%AB.html" rel="alternate" type="text/html" title="给少院新生的分享"/><published>2022-02-26T00:00:00+08:00</published><updated>2022-02-26T00:00:00+08:00</updated><id>https://meijitian.github.io/application2phd/2022/02/26/%E7%BB%99%E5%B0%91%E9%99%A2%E6%96%B0%E7%94%9F%E7%9A%84%E5%88%86%E4%BA%AB</id><content type="html" xml:base="https://meijitian.github.io/application2phd/2022/02/26/%E7%BB%99%E5%B0%91%E9%99%A2%E6%96%B0%E7%94%9F%E7%9A%84%E5%88%86%E4%BA%AB.html"><![CDATA[<p>本文是我作为 2020-2021 年国家奖学金获得者写下的经验分享，原受众为少年班学院的学弟学妹。虽然本文不是以所有可能的读者为受众写成的稿件，但考虑到我的分享也许有机会帮到更多人，我将原文分享在下面，如有疑问，欢迎联系。</p> <h2 id="读者须知">读者须知</h2> <p>“被误解是表达者的宿命。”对于这个断言，我选择的应对策略是，看内容是由什么相关经历的人表达的，因为只有这样才能筛选后吸收。在本文中，我会尽力让与我做出的断言相关的论据更加明晰。“尽信书不如无书。”我也期待读者能自行从中选取适合参考的经验，并且不被其他内容干扰。那我们开始吧！</p> <h2 id="明确需求再行动">明确需求再行动</h2> <p>我有一个常常给出的建议：明确需求再行动。我遇到很多学弟学妹问我：“我这学期 GPA 不好，会不会之后救不回来了？”“我想出国的话，GPA 需要多少？英语是不是早点考出来？”但实际上，这些问题只有一个“正确”答案，那就是——“看情况”。</p> <p>我往往会通过提出问题来明确需求、寻找解决办法，例如：“想选（在读）什么专业？想要出国为了什么？读硕士还是博士更合适？之后要 GPA 多高可以达到目标？如果纯靠 GPA 的路很难，还有什么别的办法？哪条路是最合适的？”</p> <p>关于选专业，对于迷茫没有想法的人，我会问“赚钱会是他/她生活的重要影响因素吗？什么方面的思维更好？什么样的内容是绝对不想做的？”回答出来这些问题，选择会更清晰。</p> <p>关于出国深造，也是科大学生，尤其是少院学生，常常需要思考的。在一些情况下，存在一条鄙视链：出国 &gt; 保外校 &gt; 保本校 &gt; 考研 &gt; 工作。一方面，我认为这种刻板印象是需要破除的。比如我身边就有朋友从入校就明确了本科后就业意愿，并且一步一脚印地扎实实现目标（通过实习和某些技能的倾斜）。这个朋友在我看来就是目标明确、内心坚定的正面例子。毕竟热爱科研并且能做好科研的是极少数人，在科大这个比例已经是远高于普通群体了，再高也不可能要求人人有 “科研”梦想并且都去实现梦想。另一方面，这种“层次感”也不是没有原因的。想出国就得有“不差的” GPA、英语成绩和科研；而为什么有能力的人更高比例地选择出国呢？专业在国外有更好的团队、国外就业更赚钱、国外经历回国发展可能更顺利等都可能是原因。总的来说，我并不希望学弟学妹没有理由、只是“要”出国或者“成绩好就要出国”，但很可惜，我还是遇到了不少这样的情况。</p> <p>我一直认为，每个人都只有24小时，除了投入更多时间在学习和工作之外，学会选择更应该做的事情也非常重要。这个权衡的指标就是真实需求。下面我会说一些非常个人的经历，希望给读者以启发。</p> <h2 id="个人经历">个人经历</h2> <p>我选择中科大少年班，考虑到两个因素：我高二就已经把高考内容学得差不多了 :)，和中科大选专业自由度高。我是进入大学才发现自己喜欢计算机的，花了第一年明确了专业，选择了计算机学院。这个过程中，我的背景（无竞赛）使我在学物理基础课的时候远不如其他有竞赛基础的人顺利，但一直以来帮助我的数学能力使得我在非数学专业的数学基础课相对轻松。在这个情形下，我磕磕碰碰地走过了第一年。我有“擅长”的数学都走得非常辛苦，所以我能想象到大部分人在科大的辛苦。</p> <p>大二大三是分岔路：随着我喜欢的专业课比例逐步上升，我慢慢地掌握了大学课程、包括计算机课程的思维方式（当然还有考试方式 哈哈）。我费了不少精力在课程上，但同时也一直在思考“我适不适合科研？我需要选择什么样的科研导师/组？”我获得这两个问题的答案的顺序却是反的：我找对了科研的地方，借此明确了我适合科研、喜欢科研，并且用这段经历换来了非常满意的 PhD offer：包括 CMU CS PhD 录取之外还有十几个 CS PhD offer。这种“成功”的主要原因是“科研”。</p> <p>在科研上用心思是每一个想要申请 PhD 的人最赚的选择。用心思不只是做科研本身，还有前期筛选实验室/导师，以及在科研上学会方法、最终比别人进展更好。例如，想要出国深造，申请的三封推荐信里，通常至少一封出自本校的导师，一封出自暑研的导师。在我的规划中，本校的导师至少在科研上与国际顶级科研接轨，并且关心学生、愿意帮助学生成长，大部分时间在校内科研需要能学到真本领；暑研的导师需要推荐信有力度，至少不能是籍籍无名（不然我多么优秀别人也不确定可信度），能入手的课题要有信心做出点进展（至少要让老师有成功细节可以写入推荐信）。我在这方面下了功夫，路选对了，后面耐下心工作即可。</p> <p>实际上，我的运道不算很好。疫情之下，计算机的暑研虽然仍然可以进行，但是大部分是远程，质量不如往年，暑研的数量也有所收缩。但是，我拼尽全力把项目做出了进展，在申请前投了顶会一作。这对我帮助极大。首先，我在存储方面攒下了很多经历，也明确了科研的意愿；其次，我的个人陈述有了非常好的素材，面试的时候面对问题也只需要选一些科研的成功例子出来应对即可；另外，这个过程中我形成了自己的 taste，我知道了什么样的组和什么样的教授是我想要的（往往他们也想要我这样背景的学生）；最后，不需要投稿的结果出来，对于我所在的计算机系统领域，有过真实的科研经历已经非常不同，更何况我还独立领导项目，并且克服了各种各样的困难（从个人陈述和强推可见），所以只要对口，教授们都会比较心动的（甚至不对口的教授也心动）。</p> <p>这个故事在系统领域比较成立，因为系统的背景情况是“出成果慢而且艰辛”。比如做一年多投一篇稿，被拒了之后又磨一段时间才被接收，是非常常见的。更不用说，有时候项目做到最后发现做不出有意义的成果，无疾而终也是常见的。所以对于本科生，我们通常不比拼顶会论文数（绝大多数都是 0），或者有论文但不是一作可能会大打折扣。但是在有些领域，论文数量和水平就是强相关的，那就另当别论。</p> <h2 id="少年班学院">少年班学院</h2> <p>最后，关于少年班学院，我个人是有感受到由于学院不同、班级不同，我与计算机学院的同学之间并不十分熟悉。说得通俗一点就是，往往少院计算机专业的人呆在一起，计算机学院的人呆在一起。这个现象非常好理解，从大一少院分专业前都跟少院人呆在一起，到宿舍四年都不在一起，这些客观因素一定会带来隔阂，更何况大家也只是相对地有隔阂。但当我大三渐渐地与计科同学熟悉起来，发现我错过了许多互相学习、交流的机会。这是一件我遗憾的事情，希望学弟学妹能更早地注意到这个情况。</p> <p>祝愿每个学弟学妹都走出自己的路！</p>]]></content><author><name></name></author><category term="application2phd"/><category term="经验分享"/><summary type="html"><![CDATA[本文是我作为 2020-2021 年国家奖学金获得者写下的经验分享，原受众为少年班学院的学弟学妹。虽然本文不是以所有可能的读者为受众写成的稿件，但考虑到我的分享也许有机会帮到更多人，我将原文分享在下面，如有疑问，欢迎联系。]]></summary></entry><entry><title type="html">Introduction to HHVM LRU Cache</title><link href="https://meijitian.github.io/caching/2022/02/03/Introduction-to-HHVM-LRU-Cache.html" rel="alternate" type="text/html" title="Introduction to HHVM LRU Cache"/><published>2022-02-03T00:00:00+08:00</published><updated>2022-02-03T00:00:00+08:00</updated><id>https://meijitian.github.io/caching/2022/02/03/Introduction%20to%20HHVM%20LRU%20Cache</id><content type="html" xml:base="https://meijitian.github.io/caching/2022/02/03/Introduction-to-HHVM-LRU-Cache.html"><![CDATA[<h2 id="what-you-will-see">What you will see</h2> <p>I am impressed by this 400 LOC pure implementation of LRU Cache <a href="https://github.com/facebook/hhvm/blob/master/hphp/util/concurrent-lru-cache.h">source code</a>, and decide to introduce its design through scanning the code. I believe learning HHVM LRU Cache implementation can help when needing to understand all kinds of cache implementations.</p> <ul> <li>Data structures including doubly-linked list and hash table</li> <li>APIs including <code class="language-plaintext highlighter-rouge">insert()</code>, <code class="language-plaintext highlighter-rouge">find()</code></li> <li>Thread-safe</li> <li>non-blocking (try_lock) for hit path</li> <li>Thinking questions</li> </ul> <h2 id="terms">Terms</h2> <ul> <li> <p><strong>Key</strong> Keys are hashed to metadata and searched in hash table here. Related to <code class="language-plaintext highlighter-rouge">m_key</code>, <code class="language-plaintext highlighter-rouge">key</code> in source code.</p> </li> <li> <p><strong>Object</strong> values for a Key-Value Cache or blocks for a Block Cache.</p> </li> <li> <p><strong>Eviction</strong> Eviction refers an item exiting from cache because the cache storage is full. Evictions are triggerred internally.</p> </li> <li><strong>LRU List</strong> LRU List maintains the LRU order. Related to <code class="language-plaintext highlighter-rouge">m_head</code> and <code class="language-plaintext highlighter-rouge">m_tail</code> in source code. <ul> <li><strong>Node</strong> Node refers node in LRU List, with type of <code class="language-plaintext highlighter-rouge">ListNode</code>.</li> <li><strong>Lock</strong> Locks are used for protection under concurrent accesses, such as <code class="language-plaintext highlighter-rouge">m_listMutex</code></li> </ul> </li> <li><strong>Hash Table</strong> Related to <code class="language-plaintext highlighter-rouge">HashMap</code> and <code class="language-plaintext highlighter-rouge">m_map</code> in source code</li> </ul> <h2 id="data-structures">Data structures</h2> <p>Architecture TBD :)</p> <h3 id="listnode">ListNode</h3> <p>Each node’s prev pointer is always inited into <code class="language-plaintext highlighter-rouge">OutOfListMarker</code>, which is a global const.</p> <p>The main usage of <code class="language-plaintext highlighter-rouge">OutOfListMarker</code> is to judge whether in list (<code class="language-plaintext highlighter-rouge">isInList()</code>). To elaborate more, <code class="language-plaintext highlighter-rouge">isInList()</code> is used in <code class="language-plaintext highlighter-rouge">find()</code> to avoid conflict where hash table returned node is being evicted. The conflict can happen because <code class="language-plaintext highlighter-rouge">find()</code> search hash table to get node without lock, and adding a condition can ensure thread-safety.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">struct</span> <span class="n">ListNode</span> <span class="p">{</span>
    <span class="n">ListNode</span><span class="p">()</span>
      <span class="o">:</span> <span class="n">m_prev</span><span class="p">(</span><span class="n">OutOfListMarker</span><span class="p">),</span> <span class="n">m_next</span><span class="p">(</span><span class="n">nullptr</span><span class="p">)</span>
    <span class="p">{}</span>

    <span class="n">explicit</span> <span class="n">ListNode</span><span class="p">(</span><span class="k">const</span> <span class="n">TKey</span><span class="o">&amp;</span> <span class="n">key</span><span class="p">)</span>
      <span class="o">:</span> <span class="n">m_key</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="n">m_prev</span><span class="p">(</span><span class="n">OutOfListMarker</span><span class="p">),</span> <span class="n">m_next</span><span class="p">(</span><span class="n">nullptr</span><span class="p">)</span>
    <span class="p">{}</span>

    <span class="n">TKey</span> <span class="n">m_key</span><span class="p">;</span>
    <span class="n">ListNode</span><span class="o">*</span> <span class="n">m_prev</span><span class="p">;</span>
    <span class="n">ListNode</span><span class="o">*</span> <span class="n">m_next</span><span class="p">;</span>

    <span class="n">bool</span> <span class="n">isInList</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span>
      <span class="k">return</span> <span class="n">m_prev</span> <span class="o">!=</span> <span class="n">OutOfListMarker</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">};</span>

  <span class="k">static</span> <span class="n">ListNode</span><span class="o">*</span> <span class="k">const</span> <span class="n">OutOfListMarker</span><span class="p">;</span>
</code></pre></div></div> <h3 id="hashmap">HashMap</h3> <p>Key is <code class="language-plaintext highlighter-rouge">key</code>, value include object (<code class="language-plaintext highlighter-rouge">m_value</code>) and node (<code class="language-plaintext highlighter-rouge">m_listNode</code>).</p> <p>Use Intel TBB <code class="language-plaintext highlighter-rouge">concurrent_hash_map</code> as Hash Table.</p> <p>Note that <code class="language-plaintext highlighter-rouge">const_accessor</code> and <code class="language-plaintext highlighter-rouge">accessor</code> are different. (generally read lock v.s write lock here).</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">struct</span> <span class="n">HashMapValue</span> <span class="p">{</span>
    <span class="n">HashMapValue</span><span class="p">()</span>
      <span class="o">:</span> <span class="n">m_listNode</span><span class="p">(</span><span class="n">nullptr</span><span class="p">)</span>
    <span class="p">{}</span>

    <span class="n">HashMapValue</span><span class="p">(</span><span class="k">const</span> <span class="n">TValue</span><span class="o">&amp;</span> <span class="n">value</span><span class="p">,</span> <span class="n">ListNode</span><span class="o">*</span> <span class="n">node</span><span class="p">)</span>
      <span class="o">:</span> <span class="n">m_value</span><span class="p">(</span><span class="n">value</span><span class="p">),</span> <span class="n">m_listNode</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
    <span class="p">{}</span>

    <span class="n">TValue</span> <span class="n">m_value</span><span class="p">;</span>
    <span class="n">ListNode</span><span class="o">*</span> <span class="n">m_listNode</span><span class="p">;</span>
  <span class="p">};</span>

  <span class="k">typedef</span> <span class="n">tbb</span><span class="o">::</span><span class="n">concurrent_hash_map</span><span class="o">&lt;</span><span class="n">TKey</span><span class="p">,</span> <span class="n">HashMapValue</span><span class="p">,</span> <span class="n">THash</span><span class="o">&gt;</span> <span class="n">HashMap</span><span class="p">;</span>
  <span class="k">typedef</span> <span class="kr">typename</span> <span class="n">HashMap</span><span class="o">::</span><span class="n">const_accessor</span> <span class="n">HashMapConstAccessor</span><span class="p">;</span>
  <span class="k">typedef</span> <span class="kr">typename</span> <span class="n">HashMap</span><span class="o">::</span><span class="n">accessor</span> <span class="n">HashMapAccessor</span><span class="p">;</span>
  <span class="k">typedef</span> <span class="kr">typename</span> <span class="n">HashMap</span><span class="o">::</span><span class="n">value_type</span> <span class="n">HashMapValuePair</span><span class="p">;</span>
  <span class="k">typedef</span> <span class="n">std</span><span class="o">::</span><span class="n">pair</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">TKey</span><span class="p">,</span> <span class="n">TValue</span><span class="o">&gt;</span> <span class="n">SnapshotValue</span><span class="p">;</span>
</code></pre></div></div> <p>Question: Try to use other hash table implementations (like CLHT), and test.</p> <h3 id="global-variables">Global Variables</h3> <p>Note that <code class="language-plaintext highlighter-rouge">m_size</code> is <strong>approximately</strong> equal to the number of elements in the container, and is <strong>atomic</strong>. The gap comes from concurrency, and does not become larger with time.</p> <p>For the LRU List, the “head” is the most-recently used node, and the “tail” is the least-recently used node. The list mutex must be held during both read and write.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="cm">/**
   * The maximum number of elements in the container.
   */</span>
  <span class="kt">size_t</span> <span class="n">m_maxSize</span><span class="p">;</span>

  <span class="cm">/**
   * This atomic variable is used to signal to all threads whether or not
   * eviction should be done on insert. It is approximately equal to the
   * number of elements in the container.
   */</span>
  <span class="n">std</span><span class="o">::</span><span class="n">atomic</span><span class="o">&lt;</span><span class="kt">size_t</span><span class="o">&gt;</span> <span class="n">m_size</span><span class="p">;</span>

  <span class="cm">/**
   * The underlying TBB hash map.
   */</span>
  <span class="n">HashMap</span> <span class="n">m_map</span><span class="p">;</span>

  <span class="cm">/**
   * The linked list. The "head" is the most-recently used node, and the
   * "tail" is the least-recently used node. The list mutex must be held
   * during both read and write.
   */</span>
  <span class="n">ListNode</span> <span class="n">m_head</span><span class="p">;</span>
  <span class="n">ListNode</span> <span class="n">m_tail</span><span class="p">;</span>
  <span class="k">typedef</span> <span class="n">std</span><span class="o">::</span><span class="n">mutex</span> <span class="n">ListMutex</span><span class="p">;</span>
  <span class="n">ListMutex</span> <span class="n">m_listMutex</span><span class="p">;</span>
</code></pre></div></div> <p>Question: spinlock can perform different. Replace <code class="language-plaintext highlighter-rouge">std::mutex</code> and test.</p> <p>Question: <code class="language-plaintext highlighter-rouge">m_head</code> and <code class="language-plaintext highlighter-rouge">m_tail</code> can be organized into one marker. Try? Hint: <a href="https://github.com/facebook/rocksdb/blob/main/cache/lru_cache.cc">RocksDB LRU Cache</a></p> <h2 id="apis">APIs</h2> <p>I will highlight <code class="language-plaintext highlighter-rouge">find()</code>, <code class="language-plaintext highlighter-rouge">insert()</code> and <code class="language-plaintext highlighter-rouge">evict()</code> in this section. At the end of this part, I will attach my own implementation of <code class="language-plaintext highlighter-rouge">delete()</code>. Welcome discussion!</p> <h3 id="api-overview">API Overview</h3> <p>Note that：</p> <ul> <li><code class="language-plaintext highlighter-rouge">find()</code> return value by filling ConstAccessor</li> <li><code class="language-plaintext highlighter-rouge">insert()</code> will copy key and value (from input), then insert into hash table</li> <li><code class="language-plaintext highlighter-rouge">insert()</code> will not update/replace the same key, but return false</li> <li><code class="language-plaintext highlighter-rouge">delink()</code> and <code class="language-plaintext highlighter-rouge">pushFront()</code> should be used with lock (<code class="language-plaintext highlighter-rouge">m_listMutex</code>) when called</li> </ul> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">public:</span>
	<span class="cm">/**
   * Find a value by key, and return it by filling the ConstAccessor, which
   * can be default-constructed. Returns true if the element was found, false
   * otherwise. Updates the eviction list, making the element the
   * most-recently used.
   */</span>
  <span class="n">bool</span> <span class="nf">find</span><span class="p">(</span><span class="n">ConstAccessor</span><span class="o">&amp;</span> <span class="n">ac</span><span class="p">,</span> <span class="k">const</span> <span class="n">TKey</span><span class="o">&amp;</span> <span class="n">key</span><span class="p">);</span>

  <span class="cm">/**
   * Insert a value into the container. Both the key and value will be copied.
   * The new element will put into the eviction list as the most-recently
   * used.
   *
   * If there was already an element in the container with the same key, it
   * will not be updated, and false will be returned. Otherwise, true will be
   * returned.
   */</span>
  <span class="n">bool</span> <span class="nf">insert</span><span class="p">(</span><span class="k">const</span> <span class="n">TKey</span><span class="o">&amp;</span> <span class="n">key</span><span class="p">,</span> <span class="k">const</span> <span class="n">TValue</span><span class="o">&amp;</span> <span class="n">value</span><span class="p">);</span>

  <span class="cm">/**
   * Clear the container. NOT THREAD SAFE -- do not use while other threads
   * are accessing the container.
   */</span>
  <span class="kt">void</span> <span class="nf">clear</span><span class="p">();</span>

  <span class="cm">/**
   * Get the approximate size of the container. May be slightly too low when
   * insertion is in progress.
   */</span>
  <span class="kt">size_t</span> <span class="nf">size</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">m_size</span><span class="p">.</span><span class="n">load</span><span class="p">();</span>
  <span class="err">}</span>

<span class="n">private</span><span class="o">:</span>
  <span class="cm">/**
   * Unlink a node from the list. The caller must lock the list mutex while
   * this is called.
   */</span>
  <span class="kt">void</span> <span class="nf">delink</span><span class="p">(</span><span class="n">ListNode</span><span class="o">*</span> <span class="n">node</span><span class="p">);</span>

  <span class="cm">/**
   * Add a new node to the list in the most-recently used position. The caller
   * must lock the list mutex while this is called.
   */</span>
  <span class="kt">void</span> <span class="nf">pushFront</span><span class="p">(</span><span class="n">ListNode</span><span class="o">*</span> <span class="n">node</span><span class="p">);</span>

  <span class="cm">/**
   * Evict the least-recently used item from the container. This function does
   * its own locking.
   */</span>
  <span class="kt">void</span> <span class="nf">evict</span><span class="p">();</span>
</code></pre></div></div> <h3 id="api-details">API details</h3> <h4 id="lru-list-initialization">LRU List initialization</h4> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="k">class</span> <span class="nc">TKey</span><span class="p">,</span> <span class="k">class</span> <span class="nc">TValue</span><span class="p">,</span> <span class="k">class</span> <span class="nc">THash</span><span class="p">&gt;</span>
<span class="n">ConcurrentLRUCache</span><span class="o">&lt;</span><span class="n">TKey</span><span class="p">,</span> <span class="n">TValue</span><span class="p">,</span> <span class="n">THash</span><span class="o">&gt;::</span>
<span class="n">ConcurrentLRUCache</span><span class="p">(</span><span class="kt">size_t</span> <span class="n">maxSize</span><span class="p">)</span>
  <span class="o">:</span> <span class="n">m_maxSize</span><span class="p">(</span><span class="n">maxSize</span><span class="p">),</span> <span class="n">m_size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
  <span class="n">m_map</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="o">::</span><span class="n">hardware_concurrency</span><span class="p">()</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span> <span class="c1">// it will automatically grow</span>
<span class="p">{</span>
  <span class="n">m_head</span><span class="p">.</span><span class="n">m_prev</span> <span class="o">=</span> <span class="nb">nullptr</span><span class="p">;</span>
  <span class="n">m_head</span><span class="p">.</span><span class="n">m_next</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">m_tail</span><span class="p">;</span>
  <span class="n">m_tail</span><span class="p">.</span><span class="n">m_prev</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">m_head</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <h4 id="find">find()</h4> <p>Use read lock in hash table (<code class="language-plaintext highlighter-rouge">HashMapConstAccessor</code>) and <code class="language-plaintext highlighter-rouge">try_lock</code> for LRU List to weaken lock contention problem. If <code class="language-plaintext highlighter-rouge">lock</code> true, the hit item will be delinked from list then push back to the front of the list, called promotion.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="k">class</span> <span class="nc">TKey</span><span class="p">,</span> <span class="k">class</span> <span class="nc">TValue</span><span class="p">,</span> <span class="k">class</span> <span class="nc">THash</span><span class="p">&gt;</span>
<span class="kt">bool</span> <span class="n">ConcurrentLRUCache</span><span class="o">&lt;</span><span class="n">TKey</span><span class="p">,</span> <span class="n">TValue</span><span class="p">,</span> <span class="n">THash</span><span class="o">&gt;::</span>
<span class="n">find</span><span class="p">(</span><span class="n">ConstAccessor</span><span class="o">&amp;</span> <span class="n">ac</span><span class="p">,</span> <span class="k">const</span> <span class="n">TKey</span><span class="o">&amp;</span> <span class="n">key</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">HashMapConstAccessor</span><span class="o">&amp;</span> <span class="n">hashAccessor</span> <span class="o">=</span> <span class="n">ac</span><span class="p">.</span><span class="n">m_hashAccessor</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">m_map</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">hashAccessor</span><span class="p">,</span> <span class="n">key</span><span class="p">))</span> <span class="p">{</span>
    <span class="k">return</span> <span class="nb">false</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="c1">// Acquire the lock, but don't block if it is already held</span>
  <span class="n">std</span><span class="o">::</span><span class="n">unique_lock</span><span class="o">&lt;</span><span class="n">ListMutex</span><span class="o">&gt;</span> <span class="n">lock</span><span class="p">(</span><span class="n">m_listMutex</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">try_to_lock</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">lock</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">ListNode</span><span class="o">*</span> <span class="n">node</span> <span class="o">=</span> <span class="n">hashAccessor</span><span class="o">-&gt;</span><span class="n">second</span><span class="p">.</span><span class="n">m_listNode</span><span class="p">;</span>
    <span class="c1">// The list node may be out of the list if it is in the process of being</span>
    <span class="c1">// inserted or evicted. Doing this check allows us to lock the list for</span>
    <span class="c1">// shorter periods of time.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">isInList</span><span class="p">())</span> <span class="p">{</span>
      <span class="n">delink</span><span class="p">(</span><span class="n">node</span><span class="p">);</span>
      <span class="n">pushFront</span><span class="p">(</span><span class="n">node</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">lock</span><span class="p">.</span><span class="n">unlock</span><span class="p">();</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>I personally feel that it will be better to add some <code class="language-plaintext highlighter-rouge">assert()</code> for people who want to make code contributions to debug, like me :(</p> <p>Note that <code class="language-plaintext highlighter-rouge">delink()</code> need global variable <code class="language-plaintext highlighter-rouge">OutOfListMarker</code>, and <code class="language-plaintext highlighter-rouge">pushFront()</code> need global variable <code class="language-plaintext highlighter-rouge">m_head</code>.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="k">class</span> <span class="nc">TKey</span><span class="p">,</span> <span class="k">class</span> <span class="nc">TValue</span><span class="p">,</span> <span class="k">class</span> <span class="nc">THash</span><span class="p">&gt;</span>
<span class="kr">inline</span> <span class="kt">void</span> <span class="n">ConcurrentLRUCache</span><span class="o">&lt;</span><span class="n">TKey</span><span class="p">,</span> <span class="n">TValue</span><span class="p">,</span> <span class="n">THash</span><span class="o">&gt;::</span>
<span class="n">delink</span><span class="p">(</span><span class="n">ListNode</span><span class="o">*</span> <span class="n">node</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">ListNode</span><span class="o">*</span> <span class="n">prev</span> <span class="o">=</span> <span class="n">node</span><span class="o">-&gt;</span><span class="n">m_prev</span><span class="p">;</span>
  <span class="n">ListNode</span><span class="o">*</span> <span class="n">next</span> <span class="o">=</span> <span class="n">node</span><span class="o">-&gt;</span><span class="n">m_next</span><span class="p">;</span>
  <span class="n">prev</span><span class="o">-&gt;</span><span class="n">m_next</span> <span class="o">=</span> <span class="n">next</span><span class="p">;</span>
  <span class="n">next</span><span class="o">-&gt;</span><span class="n">m_prev</span> <span class="o">=</span> <span class="n">prev</span><span class="p">;</span>
  <span class="n">node</span><span class="o">-&gt;</span><span class="n">m_prev</span> <span class="o">=</span> <span class="n">OutOfListMarker</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">template</span> <span class="o">&lt;</span><span class="k">class</span> <span class="nc">TKey</span><span class="p">,</span> <span class="k">class</span> <span class="nc">TValue</span><span class="p">,</span> <span class="k">class</span> <span class="nc">THash</span><span class="p">&gt;</span>
<span class="kr">inline</span> <span class="kt">void</span> <span class="n">ConcurrentLRUCache</span><span class="o">&lt;</span><span class="n">TKey</span><span class="p">,</span> <span class="n">TValue</span><span class="p">,</span> <span class="n">THash</span><span class="o">&gt;::</span>
<span class="n">pushFront</span><span class="p">(</span><span class="n">ListNode</span><span class="o">*</span> <span class="n">node</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">ListNode</span><span class="o">*</span> <span class="n">oldRealHead</span> <span class="o">=</span> <span class="n">m_head</span><span class="p">.</span><span class="n">m_next</span><span class="p">;</span>
  <span class="n">node</span><span class="o">-&gt;</span><span class="n">m_prev</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">m_head</span><span class="p">;</span>
  <span class="n">node</span><span class="o">-&gt;</span><span class="n">m_next</span> <span class="o">=</span> <span class="n">oldRealHead</span><span class="p">;</span>
  <span class="n">oldRealHead</span><span class="o">-&gt;</span><span class="n">m_prev</span> <span class="o">=</span> <span class="n">node</span><span class="p">;</span>
  <span class="n">m_head</span><span class="p">.</span><span class="n">m_next</span> <span class="o">=</span> <span class="n">node</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>Question: If there is no condition <code class="language-plaintext highlighter-rouge">isInList()</code> before <code class="language-plaintext highlighter-rouge">delink()</code>, what will happen?</p> <h4 id="insert--evict">insert() &amp; evict()</h4> <p><code class="language-plaintext highlighter-rouge">insert()</code> fails when key exists, or <code class="language-plaintext highlighter-rouge">m_map.insert()</code> return true. Note that at this time, accessor made a write lock on the bucket of the key.</p> <p><code class="language-plaintext highlighter-rouge">evict()</code> is only used by <code class="language-plaintext highlighter-rouge">insert()</code>, thus should be looked inside when making use of <code class="language-plaintext highlighter-rouge">evict()</code>.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="k">class</span> <span class="nc">TKey</span><span class="p">,</span> <span class="k">class</span> <span class="nc">TValue</span><span class="p">,</span> <span class="k">class</span> <span class="nc">THash</span><span class="p">&gt;</span>
<span class="kt">bool</span> <span class="n">ConcurrentLRUCache</span><span class="o">&lt;</span><span class="n">TKey</span><span class="p">,</span> <span class="n">TValue</span><span class="p">,</span> <span class="n">THash</span><span class="o">&gt;::</span>
<span class="n">insert</span><span class="p">(</span><span class="k">const</span> <span class="n">TKey</span><span class="o">&amp;</span> <span class="n">key</span><span class="p">,</span> <span class="k">const</span> <span class="n">TValue</span><span class="o">&amp;</span> <span class="n">value</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1">// Insert into the CHM</span>
  <span class="n">ListNode</span><span class="o">*</span> <span class="n">node</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ListNode</span><span class="p">(</span><span class="n">key</span><span class="p">);</span>
  <span class="n">HashMapAccessor</span> <span class="n">hashAccessor</span><span class="p">;</span>
  <span class="n">HashMapValuePair</span> <span class="n">hashMapValue</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">HashMapValue</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">node</span><span class="p">));</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">m_map</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">hashAccessor</span><span class="p">,</span> <span class="n">hashMapValue</span><span class="p">))</span> <span class="p">{</span>
    <span class="k">delete</span> <span class="n">node</span><span class="p">;</span>
    <span class="k">return</span> <span class="nb">false</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="c1">// Evict if necessary, now that we know the hashmap insertion was successful.</span>
  <span class="kt">size_t</span> <span class="n">size</span> <span class="o">=</span> <span class="n">m_size</span><span class="p">.</span><span class="n">load</span><span class="p">();</span>
  <span class="kt">bool</span> <span class="n">evictionDone</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">size</span> <span class="o">&gt;=</span> <span class="n">m_maxSize</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// The container is at (or over) capacity, so eviction needs to be done.</span>
    <span class="c1">// Do not decrement m_size, since that would cause other threads to</span>
    <span class="c1">// inappropriately omit eviction during their own inserts.</span>
    <span class="n">evict</span><span class="p">();</span>
    <span class="n">evictionDone</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="c1">// Note that we have to update the LRU list before we increment m_size, so</span>
  <span class="c1">// that other threads don't attempt to evict list items before they even</span>
  <span class="c1">// exist.</span>
  <span class="n">std</span><span class="o">::</span><span class="n">unique_lock</span><span class="o">&lt;</span><span class="n">ListMutex</span><span class="o">&gt;</span> <span class="n">lock</span><span class="p">(</span><span class="n">m_listMutex</span><span class="p">);</span>
  <span class="n">pushFront</span><span class="p">(</span><span class="n">node</span><span class="p">);</span>
  <span class="n">lock</span><span class="p">.</span><span class="n">unlock</span><span class="p">();</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">evictionDone</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">m_size</span><span class="o">++</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">size</span> <span class="o">&gt;</span> <span class="n">m_maxSize</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// It is possible for the size to temporarily exceed the maximum if there is</span>
    <span class="c1">// a heavy insert() load, once only as the cache fills. In this situation,</span>
    <span class="c1">// we have to be careful not to have every thread simultaneously attempt to</span>
    <span class="c1">// evict the extra entries, since we could end up underfilled. Instead we do</span>
    <span class="c1">// a compare-and-exchange to acquire an exclusive right to reduce the size</span>
    <span class="c1">// to a particular value.</span>
    <span class="c1">//</span>
    <span class="c1">// We could continue to evict in a loop, but if there are a lot of threads</span>
    <span class="c1">// here at the same time, that could lead to spinning. So we will just evict</span>
    <span class="c1">// one extra element per insert() until the overfill is rectified.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">m_size</span><span class="p">.</span><span class="n">compare_exchange_strong</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="p">{</span>
      <span class="n">evict</span><span class="p">();</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>Question: Will it be possible for <code class="language-plaintext highlighter-rouge">evict()</code> to fail internally?</p> <p>Question: Why <code class="language-plaintext highlighter-rouge">m_size.compare_exchange_strong(size, size - 1)</code>? Is there any other way to implement?</p> <p>Question: Why in this order: insert hash table -&gt; evict object -&gt; pushFront node? What could be wrong with pushFront ahead of insert hash table?</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="k">class</span> <span class="nc">TKey</span><span class="p">,</span> <span class="k">class</span> <span class="nc">TValue</span><span class="p">,</span> <span class="k">class</span> <span class="nc">THash</span><span class="p">&gt;</span>
<span class="kt">void</span> <span class="n">ConcurrentLRUCache</span><span class="o">&lt;</span><span class="n">TKey</span><span class="p">,</span> <span class="n">TValue</span><span class="p">,</span> <span class="n">THash</span><span class="o">&gt;::</span>
<span class="n">evict</span><span class="p">()</span> <span class="p">{</span>
  <span class="n">std</span><span class="o">::</span><span class="n">unique_lock</span><span class="o">&lt;</span><span class="n">ListMutex</span><span class="o">&gt;</span> <span class="n">lock</span><span class="p">(</span><span class="n">m_listMutex</span><span class="p">);</span>
  <span class="n">ListNode</span><span class="o">*</span> <span class="n">moribund</span> <span class="o">=</span> <span class="n">m_tail</span><span class="p">.</span><span class="n">m_prev</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">moribund</span> <span class="o">==</span> <span class="o">&amp;</span><span class="n">m_head</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// List is empty, can't evict</span>
    <span class="k">return</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="n">delink</span><span class="p">(</span><span class="n">moribund</span><span class="p">);</span>
  <span class="n">lock</span><span class="p">.</span><span class="n">unlock</span><span class="p">();</span>

  <span class="n">HashMapAccessor</span> <span class="n">hashAccessor</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">m_map</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">hashAccessor</span><span class="p">,</span> <span class="n">moribund</span><span class="o">-&gt;</span><span class="n">m_key</span><span class="p">))</span> <span class="p">{</span>
    <span class="c1">// Presumably unreachable</span>
    <span class="k">return</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="n">m_map</span><span class="p">.</span><span class="n">erase</span><span class="p">(</span><span class="n">hashAccessor</span><span class="p">);</span>
  <span class="k">delete</span> <span class="n">moribund</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>Question: When will <code class="language-plaintext highlighter-rouge">moribund == &amp;m_head</code> become true?</p> <p>Question: Will <code class="language-plaintext highlighter-rouge">!m_map.find(hashAccessor, moribund-&gt;m_key)</code> become true?</p> <h3 id="add-support-of-delete_key">Add support of delete_key()</h3> <p>It seems easy to implement <code class="language-plaintext highlighter-rouge">delete_key()</code> since it is similar to <code class="language-plaintext highlighter-rouge">insert()</code>, but there is one thing to take care.</p> <p><code class="language-plaintext highlighter-rouge">delete_key()</code> should do <code class="language-plaintext highlighter-rouge">m_size--</code> but <code class="language-plaintext highlighter-rouge">evict()</code> shouldn’t do <code class="language-plaintext highlighter-rouge">m_size++</code>, since insert maintain the consistency of <code class="language-plaintext highlighter-rouge">m_size</code>. Therefore, if user move <code class="language-plaintext highlighter-rouge">evict()</code> into public and calls it, there should add code like <code class="language-plaintext highlighter-rouge">m_size--</code>.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="k">class</span> <span class="nc">TKey</span><span class="p">,</span> <span class="k">class</span> <span class="nc">TValue</span><span class="p">,</span> <span class="k">class</span> <span class="nc">THash</span><span class="p">&gt;</span>
<span class="kt">void</span> <span class="n">ConcurrentLRUCache</span><span class="o">&lt;</span><span class="n">TKey</span><span class="p">,</span> <span class="n">TValue</span><span class="p">,</span> <span class="n">THash</span><span class="o">&gt;::</span><span class="n">delete_key</span><span class="p">(</span><span class="k">const</span> <span class="n">TKey</span><span class="o">&amp;</span> <span class="n">key</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">HashMapAccessor</span> <span class="n">hashAccessor</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">m_map</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">hashAccessor</span><span class="p">,</span> <span class="n">key</span><span class="p">))</span> <span class="p">{</span>
    <span class="k">return</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="n">std</span><span class="o">::</span><span class="n">unique_lock</span><span class="o">&lt;</span><span class="n">ListMutex</span><span class="o">&gt;</span> <span class="n">lock</span><span class="p">(</span><span class="n">m_listMutex</span><span class="p">);</span>
  <span class="n">ListNode</span><span class="o">*</span> <span class="n">node</span> <span class="o">=</span> <span class="n">hashAccessor</span><span class="o">-&gt;</span><span class="n">second</span><span class="p">.</span><span class="n">m_listNode</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">isInList</span><span class="p">())</span> <span class="p">{</span>
    <span class="k">return</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="n">delink</span><span class="p">(</span><span class="n">node</span><span class="p">);</span>
  <span class="n">lock</span><span class="p">.</span><span class="n">unlock</span><span class="p">();</span>

  <span class="n">m_map</span><span class="p">.</span><span class="n">erase</span><span class="p">(</span><span class="n">hashAccessor</span><span class="p">);</span>
  <span class="k">delete</span> <span class="n">node</span><span class="p">;</span>
  <span class="n">m_size</span><span class="o">--</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>Question: Can accessor be replaced into const accessor?</p>]]></content><author><name></name></author><category term="caching"/><category term="Cache"/><summary type="html"><![CDATA[What you will see]]></summary></entry><entry><title type="html">Experiences with CLHT, a research concurrent hash table from EPFL</title><link href="https://meijitian.github.io/caching/2022/02/02/Experiences-with-CLHT.html" rel="alternate" type="text/html" title="Experiences with CLHT, a research concurrent hash table from EPFL"/><published>2022-02-02T00:00:00+08:00</published><updated>2022-02-02T00:00:00+08:00</updated><id>https://meijitian.github.io/caching/2022/02/02/Experiences%20with%20CLHT</id><content type="html" xml:base="https://meijitian.github.io/caching/2022/02/02/Experiences-with-CLHT.html"><![CDATA[<h2 id="what-you-will-see">What you will see</h2> <ol> <li> <p>Fix CLHT into use</p> </li> <li> <p>Add <code class="language-plaintext highlighter-rouge">clht_clear()</code> to support usage</p> </li> </ol> <p>View complete code change from my <a href="https://github.com/USTCqzy/CLHT">forked repo here</a></p> <h2 id="code-change-for-usage">Code change for usage</h2> <h3 id="machine-dependent-parameters">Machine dependent parameters</h3> <p>Get info from machine (for example)</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> /proc/cpuinfo | <span class="nb">grep </span>name | <span class="nb">cut</span> <span class="nt">-f2</span> <span class="nt">-d</span>: | <span class="nb">uniq</span> <span class="nt">-c</span>
160  Intel<span class="o">(</span>R<span class="o">)</span> Xeon<span class="o">(</span>R<span class="o">)</span> Platinum 8380 CPU @ 2.30GHz

...
</code></pre></div></div> <p>Add info into <code class="language-plaintext highlighter-rouge">external/include/utils.h</code> (for example)</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#define XEONR
</span>  
<span class="p">...</span>
  <span class="c1">//machine dependent parameters</span>
<span class="cp">#ifdef __sparc__ // prior
</span><span class="p">...</span>
  
<span class="cp">#elif defined(XEONR)
#define NUMBER_OF_SOCKETS 2
#define CORES_PER_SOCKET 40
#define CACHE_LINE_SIZE 64
#define NOP_DURATION 2
</span>
  <span class="k">static</span> <span class="kt">uint8_t</span> <span class="n">__attribute__</span> <span class="p">((</span><span class="n">unused</span><span class="p">))</span> <span class="n">the_cores</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span>
    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span>
    <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span>
    <span class="mi">20</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">29</span><span class="p">,</span>
    <span class="mi">30</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">34</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">37</span><span class="p">,</span> <span class="mi">38</span><span class="p">,</span> <span class="mi">39</span><span class="p">,</span>
    <span class="mi">40</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">46</span><span class="p">,</span> <span class="mi">47</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">49</span><span class="p">,</span>
    <span class="mi">50</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">52</span><span class="p">,</span> <span class="mi">53</span><span class="p">,</span> <span class="mi">54</span><span class="p">,</span> <span class="mi">55</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">57</span><span class="p">,</span> <span class="mi">58</span><span class="p">,</span> <span class="mi">59</span><span class="p">,</span>
    <span class="mi">60</span><span class="p">,</span> <span class="mi">61</span><span class="p">,</span> <span class="mi">62</span><span class="p">,</span> <span class="mi">63</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">65</span><span class="p">,</span> <span class="mi">66</span><span class="p">,</span> <span class="mi">67</span><span class="p">,</span> <span class="mi">68</span><span class="p">,</span> <span class="mi">69</span><span class="p">,</span>
    <span class="mi">70</span><span class="p">,</span> <span class="mi">71</span><span class="p">,</span> <span class="mi">72</span><span class="p">,</span> <span class="mi">73</span><span class="p">,</span> <span class="mi">74</span><span class="p">,</span> <span class="mi">75</span><span class="p">,</span> <span class="mi">76</span><span class="p">,</span> <span class="mi">77</span><span class="p">,</span> <span class="mi">78</span><span class="p">,</span> <span class="mi">79</span><span class="p">,</span>
  <span class="p">};</span>
  <span class="k">static</span> <span class="kt">uint8_t</span> <span class="n">__attribute__</span> <span class="p">((</span><span class="n">unused</span><span class="p">))</span> <span class="n">the_sockets</span><span class="p">[]</span> <span class="o">=</span>
  <span class="p">{</span>
    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
  <span class="p">};</span>
</code></pre></div></div> <h3 id="c-support">C++ support</h3> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#ifdef __cplusplus
</span><span class="k">extern</span> <span class="s">"C"</span>
<span class="p">{</span>
<span class="cp">#endif
</span>  
<span class="p">...</span> <span class="c1">// prior declaration</span>
  
<span class="cp">#ifdef __cplusplus
</span><span class="p">}</span>
<span class="cp">#endif
</span></code></pre></div></div> <h2 id="clht_clear">clht_clear</h2> <p>for lock based version <code class="language-plaintext highlighter-rouge">src/clht_lb.c</code> or lock based &amp; resizing version <code class="language-plaintext highlighter-rouge">src/clht_lb_res.c</code></p> <p>we implemented <code class="language-plaintext highlighter-rouge">clht_clear</code> since CLHT only supports <code class="language-plaintext highlighter-rouge">clht_destroy</code> before</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span>
<span class="nf">clht_clear</span><span class="p">(</span><span class="n">clht_hashtable_t</span><span class="o">*</span> <span class="n">hashtable</span><span class="p">)</span>
<span class="p">{</span>
  <span class="kt">uint64_t</span> <span class="n">num_buckets</span> <span class="o">=</span> <span class="n">hashtable</span><span class="o">-&gt;</span><span class="n">num_buckets</span><span class="p">;</span>
  <span class="n">bucket_t</span><span class="o">*</span> <span class="n">bucket</span><span class="p">;</span>

  <span class="n">printf</span><span class="p">(</span><span class="s">"Clear number of buckets: %u</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">num_buckets</span><span class="p">);</span>
  <span class="kt">uint64_t</span> <span class="n">bin</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="n">bin</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">bin</span> <span class="o">&lt;</span> <span class="n">num_buckets</span><span class="p">;</span> <span class="n">bin</span><span class="o">++</span><span class="p">)</span>
  <span class="p">{</span>
      <span class="n">bucket</span> <span class="o">=</span> <span class="n">hashtable</span><span class="o">-&gt;</span><span class="n">table</span> <span class="o">+</span> <span class="n">bin</span><span class="p">;</span>
      <span class="kt">uint32_t</span> <span class="n">j</span><span class="p">;</span>
      <span class="k">do</span>
			<span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">ENTRIES_PER_BUCKET</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span>
        <span class="p">{</span>
          <span class="n">bucket</span><span class="o">-&gt;</span><span class="n">key</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="n">bucket</span> <span class="o">=</span> <span class="n">bucket</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>
      <span class="p">}</span>
      <span class="k">while</span> <span class="p">(</span><span class="n">bucket</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>compared with</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span>
<span class="nf">clht_destroy</span><span class="p">(</span><span class="n">clht_hashtable_t</span><span class="o">*</span> <span class="n">hashtable</span><span class="p">)</span>
<span class="p">{</span>
  <span class="n">free</span><span class="p">(</span><span class="n">hashtable</span><span class="o">-&gt;</span><span class="n">table</span><span class="p">);</span>
  <span class="n">free</span><span class="p">(</span><span class="n">hashtable</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <h2 id="reference">Reference</h2> <ol> <li>Related Publication: <a href="https://infoscience.epfl.ch/record/207109"><em>Asynchronized Concurrency: The Secret to Scaling Concurrent Search Data Structures</em></a>, Tudor David, Rachid Guerraoui, Vasileios Trigonakis (alphabetical order), ASPLOS ‘15</li> </ol>]]></content><author><name></name></author><category term="caching"/><category term="Hash Table"/><summary type="html"><![CDATA[What you will see]]></summary></entry></feed>